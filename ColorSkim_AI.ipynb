{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColorSkim Machine Learning AI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modul dasar⁡⁡\n",
    "import os\n",
    "import random\n",
    "import gc #garbage collector\n",
    "# import pandas, numpy dan tensorflow⁡\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# import daftar device terdeteksi oleh tensorflow\n",
    "from tensorflow.python.client.device_lib import list_local_devices\n",
    "\n",
    "# import utilitas umum tensorflow\n",
    "from tensorflow.config import run_functions_eagerly # type: ignore\n",
    "from tensorflow.data.experimental import enable_debug_mode # type: ignore\n",
    "\n",
    "# import pembuatan dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\"\"\"\n",
    "karena struktur objek dalam tf.data.Dataset, from_tensor_slices() \n",
    "tidak dapat dipanggil secara langsung dalam modul import\n",
    "\"\"\"\n",
    "from_tensor_slices = tf.data.Dataset.from_tensor_slices\n",
    "zip = tf.data.Dataset.zip\n",
    "\n",
    "# import preprocessing data\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "# import pipeline scikit untuk model_0\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# import layer neural network\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dense, GlobalMaxPooling1D, Bidirectional, LSTM, Dropout # type: ignore\n",
    "from tensorflow.keras.layers import Concatenate # type: ignore\n",
    "from tensorflow.keras.layers import TextVectorization # type: ignore\n",
    "from tensorflow.keras.layers import Embedding # type: ignore\n",
    "\n",
    "# import fungsi loss dan optimizer\n",
    "from tensorflow.keras.losses import BinaryCrossentropy # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "\n",
    "# import model Functional API tensorflow\n",
    "from tensorflow.keras import Model # type: ignore\n",
    "\n",
    "# import callbacks untuk tensorflow\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau # type: ignore\n",
    "\n",
    "# import model terbaik, metriks dan alat evaluasi\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from tensorflow.keras.models import load_model # type: ignore\n",
    "\n",
    "# import grafik\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.utils import plot_model # type: ignore\n",
    "from scipy.stats import binned_statistic\n",
    "\n",
    "# import display untuk menampilkan dataframe berdasar settingan tertentu (situasional)\n",
    "from IPython.display import display\n",
    "\n",
    "# import library log untuk training\n",
    "import wandb as wb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# import kunci untuk login wandb\n",
    "from rahasia import API_KEY_WANDB # type: ignore\n",
    "\n",
    "# set output tensorflow\n",
    "run_functions_eagerly(True)\n",
    "enable_debug_mode()\n",
    "\n",
    "# set matplotlib untuk menggunakan tampilan seaborn\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cek ketersediaan GPU untuk modeling\n",
    "# NVidia GeForce MX250 - office\n",
    "# NVidia GeForce GTX1060 - home\n",
    "list_local_devices()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variabel Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_MODEL_CHECKPOINT = 'colorskim_checkpoint'\n",
    "# kita akan mengatur toleransi_es sebagai fraksi (fraksi_toleransi) tertentu dari jumlah total epoch\n",
    "# dan toleransi_rlop sebagai toleransi_es dibagi dengan jumlah kesempatan (kesempatan_rlop)\n",
    "# dilakukannya reduksi pada learning_rate \n",
    "EPOCHS = 1000\n",
    "UKURAN_BATCH = 32\n",
    "FRAKSI_TOLERANSI = 0.01\n",
    "KESEMPATAN_RLOP = 5\n",
    "TOLERANSI_ES = int(EPOCHS*FRAKSI_TOLERANSI)\n",
    "TOLERANSI_RLOP = int(TOLERANSI_ES/KESEMPATAN_RLOP)\n",
    "FRAKSI_REDUKSI_LR = 0.1\n",
    "METRIK_MONITOR = 'val_accuracy'\n",
    "RANDOM_STATE = 11\n",
    "# untuk mencegah overfitting, kita akan memberikan ruang yang cukup besar \n",
    "# untuk test_data dan memperkecil porsi train_data dengan jumlah epoch\n",
    "# yang besar sehingga model masih memiliki waktu untuk melakukan\n",
    "# training pada train_data\n",
    "RASIO_TEST_TRAIN = 0.2\n",
    "\n",
    "# wandb init\n",
    "wandb = {'proyek': 'ColorSkim',\n",
    "         'user': 'jpao'}\n",
    "\n",
    "# nama model\n",
    "MODEL = ['model_0_multinomial_naive_bayes',\n",
    "         'model_1_Conv1D_vektorisasi_embedding',\n",
    "         'model_2_Conv1D_USE_embed',\n",
    "         'model_3_quadbrid_embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "Beberapa *callbacks* yang akan digunakan dalam proses *training* model diantaranya:\n",
    "\n",
    "* `WandbCallback` - *Callback* ke [wandb.ai](https://wandb.ai) untuk mencatat log dari sesi *training* model.\n",
    "* `ModelCheckpoint` - Untuk menyimpan model dengan *val_loss* terbaik dari seluruh *epoch* dalam *training* model.\n",
    "* `EarlyStopping` (ES) - *Callback* ini digunakan untuk menghentikan proses *training* model jika selama beberapa *epoch* model tidak mengalami perbaikan pada metrik *val_loss*-nya. *Callback* ini juga digunakan bersama dengan `ReduceLROnPlateau` dimana *patience* ES > *patience* RLOP.\n",
    "* `ReduceLROnPlateau` (RLOP) - *Callback* ini digunakan untuk memperkecil *learning_rate* dari model jika tidak mengalami perbaikan *val_loss* selama beberapa *epoch*.\n",
    "\n",
    "*Patience* dari ES di-set lebih tinggi dari *patience* RLOP untuk memberikan kesempatan bagi RLOP untuk memperkecil *learning_rate* beberapa kali sebelum proses *training* model dihentikan oleh ES setelah tidak berhasil mendapatkan *val_loss* yang lebih baik selama beberapa *epoch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Login ke wandb\n",
    "wb.login(key=API_KEY_WANDB)\n",
    "\n",
    "# Pembuatan fungsi callback\n",
    "def wandb_callback(data_training):\n",
    "    return WandbCallback(save_model=False, # model akan disimpan menggunakan callback ModelCheckpoint\n",
    "                         log_weights=True, # weight akan disimpan untuk visualisasi di wandb\n",
    "                         log_gradients=True, # gradient akan disimpan untuk visualisasi di wandb\n",
    "                         training_data=data_training) \n",
    "def model_checkpoint(nama_model):\n",
    "    return ModelCheckpoint(filepath=os.path.join(DIR_MODEL_CHECKPOINT, nama_model),\n",
    "                           verbose=0,\n",
    "                           monitor=METRIK_MONITOR,\n",
    "                           save_best_only=True) # model dengan 'val_loss' terbaik akan disimpan\n",
    "def early_stopping():\n",
    "    return EarlyStopping(patience=TOLERANSI_ES,\n",
    "                         monitor=METRIK_MONITOR)\n",
    "def reduce_lr_on_plateau():\n",
    "    return ReduceLROnPlateau(factor=FRAKSI_REDUKSI_LR, # pengurangan learning_rate diset sebesar 0.1 * learning_rate\n",
    "                             patience=TOLERANSI_RLOP,\n",
    "                             monitor=METRIK_MONITOR,\n",
    "                             verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Data yang dipergunakan adalah sebanyak 101,077 kata. Terdapat 2 versi data, data versi 1 hanya memiliki 56,751 kata dan data versi 2 adalah data lengkap.\n",
    "\n",
    "* Data 1: 56,751 kata, terdiri dari 34,174 kata dengan label `bukan_warna` dan 22,577 kata dengan label `warna` atau rasio 1.51 : 1 `bukan_warna` berbanding `warna`\n",
    "* Data 2: 101,077 kata, rincian menyusul....\n",
    "\n",
    "`brand`, `urut_kata` dan `total_kata` akan digunakan sebagai alternatif variabel independen tambahan dalam model tertentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Membaca data ke dalam DataFrame pandas\n",
    "Merubah kolom `urut_kata` dan 'total_kata' menjadi float32\n",
    "\"\"\"\n",
    "data = pd.read_csv('data/setengah_dataset_artikel.csv')\n",
    "data = data.astype({'urut_kata': np.float32, 'total_kata': np.float32})\n",
    "# Untuk dokumentasi, gunakan format markdown untuk rendering dataframe\n",
    "# print(data.to_markdown())\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksplorasi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribusi label dalam data\n",
    "print(data['label'].value_counts())\n",
    "data['label'].value_counts().plot(kind='bar')\n",
    "plt.gca().set_title('Distribusi Label', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribusi label dalam brand (data hanya menunjukkan 10 teratas)\n",
    "print(data[['brand', 'label']].value_counts().unstack().sort_values(by='bukan_warna', ascending=False)[:10])\n",
    "data[['brand', 'label']].value_counts().unstack().sort_values(by='bukan_warna', ascending=False)[:10].plot(kind='bar')\n",
    "plt.gca().set_title('Distribusi Label Berdasarkan Brand', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konversi Fitur dan Label ke dalam numerik\n",
    "\n",
    "Kita akan melakukan pengkonversian fitur dan label ke dalam bentuk numerik, dikarenakan jaringan saraf buatan hanya dapat bekerja dalam data numerik. \n",
    "\n",
    "Terdapat dua jenis *encoding* untuk data yang bersifat kategorikal:\n",
    "\n",
    "* `OneHotEncoder`\n",
    "* `LabelEncoder`\n",
    "\n",
    "**OneHotEncoder**\n",
    "*Encoding* ini akan merubah data satu kolom menjadi multi-kolom dengan nilai 1 dan 0 dimana jumlah kolom sama dengan jumlah kategori, seperti berikut:\n",
    "\n",
    "| brand | brand_NIK | brand_ADI | brand_SPE | brand_PIE | brand_... |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| NIK | 1 | 0 | 0 | 0 | ... |\n",
    "| SPE | 0 | 0 | 1 | 0 | ... |\n",
    "| PIE | 0 | 0 | 0 | 1 | ... |\n",
    "| ADI | 0 | 1 | 0 | 0 | ... |\n",
    "| SPE | 0 | 0 | 1 | 0 | ... |\n",
    "| ... | ... | ... | ... | ... | ... |\n",
    "\n",
    "**LabelEncoder**\n",
    "*Encoding* ini akan merubah data pada satu kolom menjadi 0, 1, 2, 3.. dstnya sesuai dengan jumlah kategorinya, seperti berikut:\n",
    "\n",
    "| brand | brand_label_encoded |\n",
    "| --- | --- |\n",
    "| NIK | 0 |\n",
    "| SPE | 1 |\n",
    "| PIE | 2 |\n",
    "| ADI | 3 |\n",
    "| SPE | 1 |\n",
    "| ... | ... |\n",
    "\n",
    "**Kapan menggunakan `OneHotEncoder` atau `LabelEncoder` dalam sebuah proses encoding?** Kita dapat menggunakan `OneHotEncoder` ketika kita tidak menginginkan suatu bentuk hubungan hirarki di dalam data kategorikal yang kita miliki. Dalam hal ini ketika kita tidak ingin jaringan saraf buatan untuk memandang ADI (3) lebih signifikan dari NIK (0) dalam hal nilainya jika dilakukan label *encoding*, maka kita dapat menggunakan `OneHotEncoder`.\n",
    "Jika kategori bersifat biner seperti 'Pria' atau 'Wanita', 'Ya' atau 'Tidak' dsbnya, penggunaan `LabelEncoder` dinilai lebih efektif.\n",
    "\n",
    "> Dengan pertimbangan di atas dan melihat struktur data kita, maka kita akan menggunakan `OneHotEncoder` untuk kolom *brand* (fitur) dan menggunakan `LabelEncoder` untuk kolom *label* (target), kecuali untuk **Model 0** yang akan menggunakan fungsi ekstraksi fitur dengan `TfIdfVectorizer` kita hanya akan menggunakan kolom 'label' yang belum di-*encode*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoding pada fitur brand\n",
    "fitur_encoder = OneHotEncoder(sparse=False)\n",
    "brand_encoded = fitur_encoder.fit_transform(data['brand'].to_numpy().reshape(-1, 1))\n",
    "df_fitur_encoded = pd.DataFrame(brand_encoded, columns=fitur_encoder.get_feature_names_out(['brand']))\n",
    "\n",
    "# LabelEncoding pada target label\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoded = label_encoder.fit_transform(data['label'])\n",
    "df_label_encoded = pd.DataFrame(label_encoded, columns=['label_encoded'])\n",
    "\n",
    "# gabungkan dengan dataframe awal\n",
    "data_encoded = data.copy()\n",
    "data_encoded = pd.concat([data_encoded,df_fitur_encoded, df_label_encoded], axis=1)\n",
    "data_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konversi Data ke dalam Train dan Test untuk Model 0\n",
    "\n",
    "Data akan dibagi ke dalam train dan test data menggunakan metode `train_test_split` dari modul *sklearn.model_selection* dengan menggunakan rasio dan keacakan yang telah ditentukan di variabel global (lihat *RASIO_TEST_TRAIN* dan *RANDOM_STATE*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan header data\n",
    "data_header = data_encoded[['kata', 'brand', 'urut_kata', 'total_kata', 'label']].columns\n",
    "\n",
    "\"\"\"\n",
    "Model 0 adalah MultinomialNB yang akan menggunakan feature_extraction TfIdfVectorizer\n",
    "dimana TfIdfVectorizer hanya dapat menerima satu kolom data yang akan diubah menjadi vector\n",
    "(angka), kecuali kita dapat menggabungkan kembali brand kata dan kolom kolom lainnya ke dalam\n",
    "satu kolom seperti['NIK GREEN 1 0 0 0 1'] alih - alih [['NIK', 'GREEN', '1', '0', '0', '0', '1']]\n",
    "Maka untuk Model 0 kita tetap akan hanya menggunakan kolom 'kata' sebagai fitur.\n",
    "kolom 'brand', 'urut_kata' 'total_kata' dan 'label' sebenarnya tidak akan \n",
    "digunakan untuk training, namun pada train_test_split ini kita akan menyimpan brand untuk \n",
    "display hasil prediksi berbanding dengan target label (ground truth)\n",
    "\"\"\"\n",
    "train_data_mnb, test_data_mnb, train_target_mnb, test_target_mnb = train_test_split(data_encoded[['kata', 'brand', 'urut_kata', 'total_kata', 'label']],\n",
    "                                                                                    data_encoded['label_encoded'],\n",
    "                                                                                    test_size=RASIO_TEST_TRAIN,\n",
    "                                                                                    random_state=RANDOM_STATE)\n",
    "\n",
    "# Untuk model lainnya kita akan menggunakan semua fitur minus 'brand', 'nama_artikel', 'label' dan 'label_encoded' .drop\n",
    "train_data, test_data, train_target, test_target = train_test_split(data_encoded.drop(['brand', 'nama_artikel', 'label', 'label_encoded'], axis=1),\n",
    "                                                                    data_encoded['label_encoded'],\n",
    "                                                                    test_size=RASIO_TEST_TRAIN,\n",
    "                                                                    random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_mnb.shape, test_data_mnb.shape, train_target_mnb.shape, test_target_mnb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, test_data.shape, train_target.shape, test_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksplorasi contoh hasil split train dan test\n",
    "train_target_unik, train_target_hitung = np.unique(train_target_mnb, return_counts=True)\n",
    "test_target_unik, test_target_hitung = np.unique(test_target_mnb, return_counts=True)\n",
    "print(f'2 data pertama di train_data_mnb:\\n{train_data_mnb.iloc[:2, 0].tolist()}\\n') # :2 menampilkan 2 data pertama, :1 hanya menampilkan kata\n",
    "print(f'2 data pertama di train_data:')\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(train_data[:2])\n",
    "print(f'\\n2 label pertama di train_target (mnb & non-mnb, sama):\\n{train_target[:2].tolist()}\\n') \n",
    "print(f'2 data pertama di test_data_mnb:\\n{test_data_mnb.iloc[:2, 0].tolist()}\\n') # :2 menampilkan 2 data pertama, :1 hanya menampilkan kata\n",
    "print(f'2 data pertama di test_data:')\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(test_data[:2])\n",
    "print(f'2 label pertama di test_target (mnb & non-mnb, sama):\\n{test_target[:2].tolist()}\\n')\n",
    "train_target_distribusi = np.column_stack((train_target_unik, train_target_hitung))\n",
    "test_target_distribusi = np.column_stack((test_target_unik, test_target_hitung))\n",
    "print(f'Distribusi label (target) di train: \\n{train_target_distribusi}\\n')\n",
    "print(f'Distribusi label (target) di test: \\n{test_target_distribusi}\\n')\n",
    "print('Dimana label 0 = bukan warna dan label 1 = warna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 0: Model Dasar\n",
    "\n",
    "Model pertama yang akan kita buat adalah model *Multinomial Naive-Bayes* yang akan mengkategorisasikan *input* ke dalam kategori *output*. *Multinomial Naive-Bayes* adalah sebuah algoritma dengan metode *supervised learning* yang paling umum digunakan dalam pengkategorisasian data tekstual.\n",
    "Pada dasarnya *Naive-Bayes* merupakan algoritma yang menghitung probabilitas dari sebuah event (*output*) berdasarkan probabilitas akumulatif kejadian dari event sebelumnya. Secara singkat algoritma ini akan mempelajari berapa probabilitas dari sebuah kata, misalkan 'ADISSAGE' adalah sebuah label `bukan_warna` berdasarkan probabilitas kejadian 'ADISSAGE' adalah `bukan_warna` pada event - event sebelumnya.\n",
    "\n",
    "Formula dari probabilitias algoritma *Naive-Bayes*:\n",
    "\n",
    "$P(A|B) = \\frac{P(A) * P(B|A)}{P(B)}$\n",
    "\n",
    "Sebelum melakukan training menggunakan algoritma *Multinomial Naive-Bayes* kita perlu untuk merubah data kata menjadi bentuk numerik yang kali ini akan dikonversi menggunakan metode TF-IDF (*Term Frequency-Inverse Document Frequency*). TF-IDF sendiri merupakan metode yang akan berusaha memvaluasi nilai relevansi dan frekuensi dari sebuah kata dalam sekumpulan dokumen. *Term Frequency* merujuk pada seberapa sering sebuah kata muncul dalam 1 dokumen, sedangkan *Inverse Document Frequency* adalah perhitungan logaritma dari jumlah seluruh dokumen dibagi dengan jumlah dokumen dengan kata yang dimaksud terdapat di dalamnya. Hasil perhitungan dari TF dan IDF ini akan dikalikan untuk mendapatkan nilai dari seberapa sering dan seberapa relevan nilai dari sebuah kata. Misalkan 'ADISSAGE' sering muncul dalam 1 dokumen tapi tidak terlalu banyak muncul di dokumen - dokumen lainnya, maka hal ini dapat mengindikasikan bahwa kata 'ADISSAGE' mungkin memiliki relevansi yang tinggi dalam kategorisasi sebuah dokumen, sebaliknya jika kata 'WHITE' sering muncul di 1 dokumen dan juga sering muncul di dokumen - dokumen lainnya, maka kata 'WHITE' ini mungkin merupakan sebuah kata yang umum dan memiliki nilai relevansi yang rendah dalam pengkategorisasian sebuah dokumen.\n",
    "\n",
    "Untuk lebih lengkapnya mengenai *Naive-Bayes* dan TF-IDF dapat merujuk pada sumber berikut:\n",
    "\n",
    "* https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c\n",
    "* https://monkeylearn.com/blog/what-is-tf-idf/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat pipeline untuk mengubah kata ke dalam tf-idf\n",
    "model_0 = Pipeline([\n",
    "    (\"tf-idf\", TfidfVectorizer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit pipeline dengan data training\n",
    "model_0.fit(X=np.squeeze(train_data_mnb.iloc[:, 0]), y=train_target_mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi model_0 pada data test\n",
    "skor_model_0 = model_0.score(X=np.squeeze(test_data_mnb.iloc[:, 0]), y=test_target_mnb)\n",
    "skor_model_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksplorasi Hasil Model 0\n",
    "Pada hasil training dengan menggunakan model algoritma *Multinomial Naive-Bayes* kita mendapatkan akurasi sebesar ~99.22%\n",
    "\n",
    "Secara sekilas model yang pertama ini (model 0) memberikan akurasi yang sangat tinggi dalam membedakan kata `warna` dan `bukan_warna`. Namun secara brand speisifik, akurasi ini mungkin akan lebih buruk karena di beberapa brand terutama 'PUM' kita dapat menjumpai artikel dengan nama misalkan 'PUMA XTG WOVEN PANTS PUMA BLACK-PUMA WHITE' dimana kata PUMA pertama adalah `bukan_warna` namun kata PUMA kedua dan ketiga adalah bagian dari `warna`.\n",
    "\n",
    "Dengan demikian, nanti kita mungkin akan mengulas lebih mendalam model pertama ini menggunakan dataset yang dipisahkan berdasar brand. Untuk sementara kita akan melanjutkan mengembangkan model - model alternatif untuk pemisahan `bukan_warna` dan `warna` dari nama artikel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat prediksi menggunakan data test\n",
    "pred_model_0 = model_0.predict(np.squeeze(test_data_mnb.iloc[:, 0]))\n",
    "pred_model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat fungsi dasar untuk menghitung accuray, precision, recall, f1-score\n",
    "def hitung_metrik(target, prediksi):\n",
    "    \"\"\"\n",
    "    Menghitung accuracy, precision, recall dan f1-score dari model klasifikasi biner\n",
    "    \n",
    "    Args:\n",
    "        target: label yang sebenarnya dalam bentuk 1D array\n",
    "        prediksi: label yang diprediksi dalam bentuk 1D array\n",
    "        \n",
    "    Returns:\n",
    "        nilai accuracy, precision, recall dan f1-score dalam bentuk dictionary\n",
    "    \"\"\"\n",
    "    # Menghitung akurasi model\n",
    "    model_akurasi = accuracy_score(target, prediksi)\n",
    "    # Menghitung precision, recall, f1-score dan support dari model\n",
    "    model_presisi, model_recall, model_f1, _ = precision_recall_fscore_support(target, prediksi, average='weighted')\n",
    "    \n",
    "    hasil_model = {'akurasi': model_akurasi,\n",
    "                   'presisi': model_presisi,\n",
    "                   'recall': model_recall,\n",
    "                   'f1-score': model_f1}\n",
    "    \n",
    "    return hasil_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung metrik dari model_0\n",
    "model_0_metrik = hitung_metrik(target=test_target_mnb, \n",
    "                               prediksi=pred_model_0)\n",
    "model_0_metrik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Akurasi** merupakan metrik yang menghitung jumlah prediksi yang benar dibanding total jumlah label yang dijadikan evaluasi (test data, bukan training data).\n",
    "\n",
    "$\\frac{\\text{prediksi benar}}{\\text{total prediksi}}$\n",
    "\n",
    "**Presisi** merupakan metrik yang menghitung *true positive* berbanding dengan *true positive* dan *false positive*\n",
    "\n",
    "$\\frac{\\text{true positive}}{\\text{true positive } + \\text{ false positive}}$\n",
    "\n",
    "**Recall** merupakan metrik yang menghitung *true positive* berbanding dengan *true positive* dan *false negative*\n",
    "\n",
    "$\\frac{\\text{true positive}}{\\text{true positive } + \\text{ false negative}}$\n",
    "\n",
    "**f1-score** merupakan metrik yang mengabungkan presisi dan recall\n",
    "\n",
    "$2 * \\frac{\\text{presisi } * \\text{ recall}}{\\text{presisi } + \\text{ recall}}$\n",
    "\n",
    "Dimana:\n",
    "\n",
    "* True Positive (TP): Prediksi `warna` pada target label `warna`\n",
    "* False Positive (FP): Prediksi `warna` pada target label `bukan_warna`\n",
    "* True Negative (TN): Prediksi `bukan_warna` pada target label `bukan_warna`\n",
    "* False Negative (FN): Prediksi `bukan_warna` pada target label `warna`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat confusion matrix untuk prediksi model_0\n",
    "# cf_matrix = confusion_matrix(test_target_mnb, pred_model_0)\n",
    "\n",
    "# Menampilkan confusion matrix menggunakan seaborn\n",
    "# ax = sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "\n",
    "# ax.set_title(f'Confusion Matrix Model 0 - Akurasi {skor_model_0:.2%}')\n",
    "# ax.set_xlabel('Prediksi')\n",
    "# ax.set_ylabel('Label')\n",
    "\n",
    "# label tick\n",
    "# ax.xaxis.set_ticklabels(['bukan_warna', 'warna'])\n",
    "# ax.yaxis.set_ticklabels(['bukan_warna', 'warna'])\n",
    "\n",
    "# Tampilkan\n",
    "# plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Merubah confusion matrix ke dalam fungsi\n",
    "\"\"\"\n",
    "def plot_conf_matrix(target_label, \n",
    "                     prediksi_label, \n",
    "                     nama_model,\n",
    "                     akurasi, \n",
    "                     label_titik_x, \n",
    "                     label_titik_y):\n",
    "    \"\"\"\n",
    "    Fungsi ini akan menampilkan matrix confusion untuk perbandingan\n",
    "    target label dan prediksi label dan memahami seberapa kesulitan\n",
    "    sebuah model melakukan prediksi\n",
    "    \n",
    "    Args:\n",
    "        target_label (list atau 1D-array): label yang sebenarnya dalam bentuk 1D array\n",
    "        prediksi_label (list atau 1D-array): label yang diprediksi dalam bentuk 1D array\n",
    "        akurasi (float): akurasi model dalam bentuk float\n",
    "        label_titik_x dan label_titik_y, keduanya merupakan list dari sekumpulan\n",
    "        string dan harus dalam vector shape yang sama\n",
    "        label_titik_x (list str): label untuk x-axis dalam bentuk list\n",
    "        label_titik_y (list str): label untuk y-axis dalam bentuk list\n",
    "        \n",
    "    Returns:\n",
    "        plot_confusion_matrix\n",
    "    \"\"\"\n",
    "    # Membuat confusion matrix\n",
    "    cf_matrix = confusion_matrix(target_label,\n",
    "                                 prediksi_label)\n",
    "    # Pengaturan confusion_matrix menggunakan seaborn\n",
    "    plot_confusion_matrix = sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plot_confusion_matrix.set_title(f'Confusion Matrix\\n{nama_model}\\nAkurasi {akurasi:.2%}', fontsize=18)\n",
    "    plot_confusion_matrix.set_xlabel('Prediksi Label')\n",
    "    plot_confusion_matrix.set_ylabel('Target Label')\n",
    "    plot_confusion_matrix.xaxis.set_ticklabels(label_titik_x)\n",
    "    plot_confusion_matrix.yaxis.set_ticklabels(label_titik_y)\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan confusion matrix untuk model_0\n",
    "plot_conf_matrix(target_label=test_target_mnb,\n",
    "                 prediksi_label=pred_model_0,\n",
    "                 nama_model='Model 0 Multinomial Naive Bayes',\n",
    "                 akurasi=model_0_metrik['akurasi'],\n",
    "                 label_titik_x=['bukan_warna', 'warna'],\n",
    "                 label_titik_y=['bukan_warna', 'warna'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada tabel *Confusion Matrix* di atas kita dapat melihat bahwa Model 0 berhasil memprediksi secara tepat 6,785 kata dengan label `bukan_warna` dan 4,477 kata dengan label `warna`.\n",
    "\n",
    "Terdapat setidaknya 55 kata yang merupakan `warna` namun diprediksi oleh Model 0 sebagai `bukan_warna` dan 34 kata yang merupakan `bukan_warna` namun diprediksi oleh Model 0 sebagai `warna`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat fungsi untuk menampilkan kesalahan model dalam dataframe\n",
    "def df_kesalahan_prediksi(label_encoder, \n",
    "                          test_data, \n",
    "                          prediksi, \n",
    "                          probabilitas_prediksi=None, \n",
    "                          order_ulang_header=None):\n",
    "    \"\"\"\n",
    "    (penting) Fungsi ini akan menerima objek label encoder sklearn, set test_data\n",
    "    (penting) sebelum modifikasi encoding fitur dan label, prediksi dari model\n",
    "    (penting) serta urutan order_ulang_header jika diperlukan\n",
    "    \n",
    "    Args:\n",
    "        label_encoder (obyek LabelEncoder sklear.preprocessing): obyek label encoder dari sklearn.preprocessing\n",
    "        test_data (pd.DataFrame): dataframe lengkap sebelum modifikasi fitur dan label\n",
    "        prediksi (tf.Tensor): tensor dengan shape 1 dimensi yang memuat prediksi model\n",
    "        order_ulang_header (list): list dengan urutan header yang diinginkan\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame yang diprint dengan format markdown\n",
    "    \"\"\"\n",
    "    inverse_label_encoder = list(label_encoder.inverse_transform([0, 1]))\n",
    "    \n",
    "    if order_ulang_header is None:\n",
    "        data_final = pd.DataFrame(test_data)\n",
    "    elif type(order_ulang_header) is list:\n",
    "        data_final = pd.DataFrame(test_data)[order_ulang_header]\n",
    "    else:\n",
    "        raise TypeError('order_ulang_header harus berupa list')\n",
    "    \n",
    "    kolom_pred = pd.DataFrame(np.int8(prediksi), columns=['prediksi'])\n",
    "    kolom_prob_pred = pd.DataFrame(probabilitas_prediksi, columns=['probabilitas']) * 100\n",
    "    data_final['prediksi'] = kolom_pred.iloc[:, 0].tolist()\n",
    "    if probabilitas_prediksi is not None:\n",
    "        data_final['probabilitas'] = kolom_prob_pred.iloc[:, 0].tolist()\n",
    "        data_final['probabilitas'] = data_final['probabilitas'].round(2).astype(str) + '%'\n",
    "    data_final['prediksi'] = data_final['prediksi'].astype(int).map(lambda x: inverse_label_encoder[x])\n",
    "    data_final = data_final.loc[data_final['label'] != data_final['prediksi']]\n",
    "    with pd.option_context('display.max_rows', None):\n",
    "        print(data_final.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan kesalahan prediksi \n",
    "df_kesalahan_prediksi(label_encoder=label_encoder,\n",
    "                      test_data=test_data_mnb,\n",
    "                      prediksi=pred_model_0,\n",
    "                      order_ulang_header=['brand', \n",
    "                                          'kata', \n",
    "                                          'urut_kata', \n",
    "                                          'total_kata', \n",
    "                                          'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Conv1D dengan Embedding\n",
    "\n",
    "`Conv1D` atau *1-dimension convolution* merupakan satu jenis layer dari layer convolution yang umumnya digunakan untuk mengekstrak fitur penting dari input data.\n",
    "\n",
    "Meskipun umumnya jaringan saraf tiruan *convolution* digunakan untuk klasifikasi gambar (`Conv2D`) pada pembelajaran *image recognition*, tidak jarang juga `Conv1D` dipergunakan dalam *natural language processing* atau *time series forecasting*.\n",
    "\n",
    "Layer ini pada prinsipnya menggunakan *kernel_size*, *padding* dan juga *stride* untuk menciptakan sebuah jendela yang akan men-*scan* input matrix atau vektor secara perlahan dan melakukan *pooling* (*min*, *max* atau *average pooling*) untuk mengekstrak nilai yang menjadi fitur penting dari input data.\n",
    "\n",
    "<div>\n",
    "<center><img src=\"gambar/convlayer.gif\" width=300/></center>\n",
    "</div>\n",
    "\n",
    "*contoh `Conv2D` pada jaringan saraf tiruan untuk klasifikasi biner/multiclass dari input gambar*\n",
    "\n",
    "Lebih lanjut mengenai jaringan saraf tiruan *convolution* (convolutional neural network) dapat merujuk pada [CNN Explainer](https://poloclub.github.io/cnn-explainer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vektorisasi dan Embedding Kata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Membuat Lapisan Vektorisasi Kata\n",
    "\n",
    "Vektorisasi sebenarnya merupakan proses yang cukup sederhana yang merubah kata menjadi representasi numerik berdasarkan total jumlah kata dalam *vocabulary* dari input data.\n",
    "\n",
    "Di lapisan vektorisasi ini sebenarnya kita melakukan beberapa proses pengolahan terhadap teks yang bersifat opsional, diantaranya:\n",
    "\n",
    "* Standarisasi kata, merubah semua kata menjadi *lowercase* dan menghilangkan tanda baca (*punctuation*)\n",
    "* Split setiap input teks menjadi per kata (untuk input yang berupa kalimat)\n",
    "* Pembentukan *ngrams* pada *corpus*. Apa itu [*ngrams*](https://en.wikipedia.org/wiki/N-gram) dan [*text corpus*](https://en.wikipedia.org/wiki/Text_corpus).\n",
    "* Indeksasi token (kata)\n",
    "* Transformasi setiap input menggunakan indeksasi token untuk menghasilkan vektor integer atau vektor angka *float*\n",
    "\n",
    "Sedangkan *embedding* adalah proses lebih lanjut setelah vektorisasi kata ke dalam representasi numerik. Pada dasarnya embedding adalah sebuah lapisan yang akan memberikan kemampuan untuk menyimpan bobot awal (*initial weight*) dan juga bobot yang nilainya akan di*update* selama proses *training* untuk kata dalam input data.\n",
    "\n",
    "Sebenarnya tujuan dari proses *embedding* adalah untuk merubah kata per kata dalam sebuah kalimat dalam satu representasi vektor dengan panjang yang sama (dalam kasus *universal sentence embedding* adalah vektor dengan panjang 512) dan merata - ratakan nilai dari kesemua vektor per kata dalam kalimat menjadi satu vektor yang digunakan sebagai acuan klasifikasi, pengelompokan (clustring) atau deteksi.\n",
    "\n",
    "Meskipun dalam kasus ColorSkim ini yang coba kita lakukan adalah melakukan klasifikasi per kata dan bukan merupakan klasifikasi per kalimat, proses *embedding* masih dapat menjadi satu faktor yang penting dalam melakukan update bobot (*weights*) untuk setiap *neuron* di dalam lapisan model yang dipergunakan melalui proses *backpropagation*.\n",
    "\n",
    "<div>\n",
    "<center><img src=\"gambar/embedding.png\" width=250/></center>\n",
    "</div>\n",
    "\n",
    "Pada akhir proses training, bobot dari suatu kata sudah melalui beberapa ratus putaran *training* (*epoch*) dari jaringan saraf tiruan dan diharapkan sudah memiliki nilai yang lebih akurat untuk merepresentasikan keadaan (*state*) dari suatu kata terhadap kategori kata atau kalimat yang menjadi target dari proses *training*.\n",
    "\n",
    "Lebih lengkapnya dapat merujuk pada link berikut:\n",
    "\n",
    "- Lapisan Vektorisasi Teks: https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
    "- Lapisan Embedding Teks: https://www.tensorflow.org/text/guide/word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jumlah data (kata) dalam train_data\n",
    "print(f'jumlah data: {len(train_data.kata)}\\n')\n",
    "train_data.kata[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jumlah data unik (kata unik) dalam train_data[:, 0]\n",
    "jumlah_kata_train = len(np.unique(train_data.kata))\n",
    "jumlah_kata_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat lapisan vektorisasi kata\n",
    "lapisan_vektorisasi = TextVectorization(max_tokens=jumlah_kata_train,\n",
    "                                        output_sequence_length=1,\n",
    "                                        standardize='lower_and_strip_punctuation',\n",
    "                                        name='lapisan_vektorisasi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengadaptasikan lapisan vektorisasi ke dalam train_kata\n",
    "lapisan_vektorisasi.adapt(train_data.kata.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uji vektorisasi kata\n",
    "target_kata = random.choice(train_data.kata.tolist())\n",
    "print(f'Kata:\\n{target_kata}\\n')\n",
    "print(f'Kata setelah vektorisasi:\\n{lapisan_vektorisasi([target_kata])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# konfigurasi lapisan vektorisasi\n",
    "lapisan_vektorisasi.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jumlah vocabulary dalam lapisan_vektorisasi\n",
    "jumlah_vocab = lapisan_vektorisasi.get_vocabulary()\n",
    "len(jumlah_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Membuat Lapisan Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat lapisan embedding kata\n",
    "lapisan_embedding = Embedding(input_dim=len(jumlah_vocab),\n",
    "                              output_dim=UKURAN_BATCH,\n",
    "                              mask_zero=True,\n",
    "                              name='lapisan_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh vektorisasi dan embedding\n",
    "print(f'Kata sebelum vektorisasi:\\n{target_kata}\\n')\n",
    "kata_tervektor = lapisan_vektorisasi([target_kata])\n",
    "print(f'\\nKata sesudah vektorisasi (sebelum embedding):\\n{kata_tervektor}\\n')\n",
    "kata_terembed = lapisan_embedding(kata_tervektor)\n",
    "print(f'\\nKata setelah embedding:\\n{kata_terembed}\\n')\n",
    "print(f'Shape dari kata setelah embedding:\\n{kata_terembed.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membuat TensorFlow Dataset, Batching dan Prefetching\n",
    "\n",
    "Pada bagian ini kita akan merubah data menjadi *dataset* dan menerapkan *batching* serta *prefetching* pada dataset untuk mempercepat performa *training* model.\n",
    "\n",
    "<div>\n",
    "<center><img src=\"gambar/prefetched.jpg\" width=700/></center>\n",
    "</div>\n",
    "\n",
    "Lihat https://www.tensorflow.org/guide/data_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat TensorFlow dataset\n",
    "train_kata_dataset = from_tensor_slices((train_data.iloc[:, 0], train_target))\n",
    "test_kata_dataset = from_tensor_slices((test_data.iloc[:, 0], test_target))\n",
    "\n",
    "train_kata_dataset, test_kata_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat TensorSliceDataset menjadi prefetched dataset\n",
    "train_kata_dataset = train_kata_dataset.batch(UKURAN_BATCH).prefetch(tf.data.AUTOTUNE)\n",
    "test_kata_dataset = test_kata_dataset.batch(UKURAN_BATCH).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_kata_dataset, test_kata_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membangun dan Menjalankan Training Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jika folder dengan path 'colorskim_checkpoint/{model.name}' sudah ada, maka skip fit model \n",
    "# untuk menghemat waktu pengembangan dan hanya load model yang sudah ada dalam folder tersebut saja\n",
    "if not os.path.isdir(f'colorskim_checkpoint/{MODEL[1]}'):\n",
    "        # set random.set_seed untuk konsistensi keacakan\n",
    "        tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "        # * Membuat model_1 dengan layer Conv1D dan lapisan vektorisasi serta embedding input kata\n",
    "        inputs = Input(shape=(1,), \n",
    "                       dtype=tf.string, \n",
    "                       name='lapisan_input')\n",
    "        lapisan_vektor = lapisan_vektorisasi(inputs)\n",
    "        lapisan_embed = lapisan_embedding(lapisan_vektor)\n",
    "        x = Conv1D(filters=UKURAN_BATCH, \n",
    "                   kernel_size=5, \n",
    "                   padding='same', \n",
    "                   activation='relu',\n",
    "                   name='lapisan_konvolusional_1_dimensi')(lapisan_embed)\n",
    "        x = GlobalMaxPooling1D(name='lapisan_max_pool')(x)\n",
    "        outputs = Dense(units=1, \n",
    "                        activation='sigmoid', \n",
    "                        name='lapisan_output')(x)\n",
    "        model_1 = Model(inputs=inputs, \n",
    "                        outputs=outputs, \n",
    "                        name=MODEL[1])\n",
    "\n",
    "        # Compile\n",
    "        model_1.compile(loss=BinaryCrossentropy(),\n",
    "                        optimizer=Adam(),\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "        # Setup wandb init dan config\n",
    "        wb.init(project=wandb['proyek'],\n",
    "                entity=wandb['user'],\n",
    "                name=model_1.name,\n",
    "                config={'epochs': EPOCHS,\n",
    "                        'n_layers': len(model_1.layers)})\n",
    "\n",
    "        \n",
    "        # Fit model_1\n",
    "        model_1.fit(train_kata_dataset,\n",
    "                    epochs=wb.config.epochs,\n",
    "                    validation_data=test_kata_dataset,\n",
    "                    callbacks=[wandb_callback(train_kata_dataset),\n",
    "                               model_checkpoint(model_1.name),\n",
    "                               early_stopping(),\n",
    "                               reduce_lr_on_plateau()])\n",
    "        \n",
    "        # tutup logging wandb\n",
    "        wb.finish()\n",
    "else:\n",
    "        # load model_1\n",
    "        model_1 = load_model(f'colorskim_checkpoint/{MODEL[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ringkasan dari model_1\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model_1\n",
    "plot_model(model_1, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksplorasi Hasil Model 1\n",
    "\n",
    "Setelah proses *training* pada model_1 yang terhenti di epoch 14 setelah melalui beberapa kali reduksi *learning_rate* namun dengan *val_accuracy* yang tidak meningkat setelah melalui sejumlah toleransi epoch dari `EarlyStopping` *callbacks*, kita mendapatkan *val_accuracy* terakhir di 99.21%.\n",
    "Di bagian bawah kita akan melakukan beberapa evaluasi dari hasil *training* model_1:\n",
    "\n",
    "1. Evaluasi *val_loss* dan *val_accuracy* model_1\n",
    "2. Memuat model dengan *val_accuracy* terbaik selama *training* model_1 dan lakukan evaluasi\n",
    "3. Membuat contoh prediksi dengan model terbaik selama *training* model_1\n",
    "4. Hitung metrik dari model terbaik selama *training* model_1\n",
    "5. *Plot confusion matrix* dari model terbaik selama *training* model_1\n",
    "6. Tampilkan *False Negative* dan *False Positive* dari model terbaik selama *training* model_1 dalam dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi model_1\n",
    "model_1.evaluate(test_kata_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat prodeksi menggunakan model_1\n",
    "model_1_pred_prob = tf.squeeze(model_1.predict(test_kata_dataset))\n",
    "model_1_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pembulatan probabilitas prediksi model_1\n",
    "model_1_pred = tf.round(model_1_pred_prob)\n",
    "model_1_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung metriks dari model_1\n",
    "model_1_metrik = hitung_metrik(target=test_target,\n",
    "                               prediksi=model_1_pred)\n",
    "model_1_metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ⁡⁣⁢Membuat fungsi untuk plot residual dari model regresi logistik⁡\n",
    "def residual_plot_logr(test_target, \n",
    "                       model, \n",
    "                       nama_model,\n",
    "                       model_akurasi, \n",
    "                       probabilitas_prediksi_model, \n",
    "                       jumlah_bin=100, \n",
    "                       rentang=[0, 1]):\n",
    "    \"\"\"\n",
    "    Fungsi ini akan menciptakan residual plot untuk logistik regresi dari permodelan\n",
    "    \n",
    "    Args:\n",
    "        test_target (np.ndarray): target dari test data dalama bentuk 𝟭D numpy array\n",
    "        model (keras.Model): model hasil training jaringan saraf tiruan\n",
    "        nama_model (str): nama model dalam string untuk ditampilkan di judul plot\n",
    "        model_akurasi (float): akurasi model\n",
    "        probabilitas_prediksi_model (np.ndarray): probabilitas prediksi model dalam bentuk 𝟭D numpy array\n",
    "        jumlah_bin (int): jumlah bin yang akan digunakan untuk plot sepanjang axis x\n",
    "        rentang (list): rentang yang akan digunakan di axis x\n",
    "        \n",
    "    Returns:\n",
    "        residual_plot (matplotlib.pyplot.scatter): plot residual dari model regresi logistik⁡⁡\n",
    "    \"\"\"\n",
    "    \n",
    "    # fungsi internal untuk menjumlahkan residu dalam kelompok bin tertentu\n",
    "    def func(residu):\n",
    "        y = np.sum(residu)\n",
    "        return y\n",
    "\n",
    "    axis_x = [langkah_x/jumlah_bin for langkah_x in range(jumlah_bin+1)]\n",
    "\n",
    "    residual = test_target - probabilitas_prediksi_model\n",
    "    bin_residual = binned_statistic(residual, residual, statistic=func, bins=jumlah_bin+1, range=rentang)[0]\n",
    "    plt.scatter(axis_x, bin_residual, c='r')\n",
    "    plt.title(f'Residual Regresi Logistik\\n{nama_model}\\nAkurasi: {model_akurasi:.2%}',\n",
    "              fontsize=14)\n",
    "    plt.xlabel('Target Label dalam Bin')\n",
    "    plt.ylabel('Residual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residual dari model 1\n",
    "residual_plot_logr(test_target, model_1, MODEL[1], model_1_metrik['akurasi'], model_1_pred_prob)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan confusion matrix dari model_1\n",
    "plot_conf_matrix(target_label=test_target,\n",
    "                 prediksi_label=model_1_pred,\n",
    "                 nama_model='Model Conv1D dengan Vektorisasi Embedding',\n",
    "                 akurasi=model_1_metrik['akurasi'],\n",
    "                 label_titik_x=['bukan_warna', 'warna'],\n",
    "                 label_titik_y=['bukan_warna', 'warna'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan kesalahan prediksi \n",
    "df_kesalahan_prediksi(label_encoder=label_encoder,\n",
    "                      test_data=test_data_mnb,\n",
    "                      prediksi=model_1_pred,\n",
    "                      probabilitas_prediksi=model_1_pred_prob,\n",
    "                      order_ulang_header=['brand', \n",
    "                                          'kata', \n",
    "                                          'urut_kata', \n",
    "                                          'total_kata', \n",
    "                                          'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selesai dengan model 1, bersihkan memori di GPU terkait model_1\n",
    "del model_1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Transfer Learning pretrained feature exraction menggunakan Universal Sentence Encoder (USE)\n",
    "\n",
    "Pada bagian ini kita akan mencoba untuk melakukan training pada data menggunakan lapisan *feature extraction* yang sudah ada dan sudah dilatih pada dataset tertentu. \n",
    "Proses embedding secara umum memiliki beberapa kelemahan diantaranya:\n",
    "\n",
    "1. Kehilangan informasi, dimana dalam kasus kalimat \"Produk ini bagus\" dan kalimat \"Ini\" menggunakan rerata vektor memiliki tingkat kemiripan yg cukup tinggi meskipun keduanya merupakan kalimat yang memiliki makna cukup berbeda\n",
    "2. Tidak memandang urutan, dimana kalimat \"Makan ikan menggunakan sendok\" dan kalimat \"Makan sendok menggunakan ikan\" akan memiliki kemiripan vektor 100%\n",
    "\n",
    "Kita dapat menghindari permasalahan ini misalkan dengan menerapkan *feature engineering* untuk membuat input menjadi semakin kompleks dan menghindari masalah yang mungkin timbul dari proses *embedding*, namun hal ini dapat melibatkan beberapa proses seperti menghilangkan *stop-words*, pembobotan menggunakan TF-IDF, menambahkan *ngrams* untuk mendapatkan posisi kata dalam kalimat, penumpukan lapisan *MaxPooling* dan lain sebagainya.\n",
    "\n",
    "Universal Sentence Encoder merupakan suatu lapisan yang sudah melakukan hampir kesemua proses ini dalam proses *embedding* input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sama seperti model_1 jika sudah pernah ditrain dan ada model yang tersimpan\n",
    "# seharusnya bagian ini bisa diskip, namun untuk kepentingan demonstrasi pada \n",
    "# bagian selanjutnya, kita akan tetap mendownload USE model\n",
    "# dan menghapusnya setelah demonstrasi jika model sudah ditrain dan disimpan\n",
    "# Jika tidak, biarkan tf_hub_embedding sampai model_2 selesai\n",
    "# hapus tf_hub_embedding dan model_2 pada akhir sesi model_2\n",
    "# Mengunduh pretrained USE dari tensorflow hub\n",
    "tf_hub_embedding = hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder/4',\n",
    "                                  trainable=False,\n",
    "                                  name='lapisan_embedding_USE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melakukan tes pretrained embedding pada contoh kata\n",
    "kata_acak = random.choice(train_data_mnb['kata'].tolist())\n",
    "print(f'Kata acak:\\n {kata_acak}')\n",
    "kata_embed_pretrain = tf_hub_embedding([kata_acak])\n",
    "print(f'\\nKata setelah embed dengan USE:\\n{kata_embed_pretrain[0][:30]}\\n')\n",
    "print(f'Panjang dari kata setelah embedding: {len(kata_embed_pretrain[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membangun dan Menjalankan Training Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jika folder dengan path 'colorskim_checkpoint/{model.name}' sudah ada, maka skip fit model \n",
    "# untuk menghemat waktu pengembangan dan hanya load model yang sudah ada dalam folder tersebut \n",
    "# saja\n",
    "# Terutama untuk model_2 yang sangat resource intensif, baik data yang didownload dari tfhub.dev\n",
    "# maupun output model dari training yang cukup besar (~1GB) berbanding model_1 yang hanya menghasilkan\n",
    "# model dengan ukuran 2MB, maka untuk output model dari model_2 akan disimpan di remote data version\n",
    "# control dengan modul dvc atau dapat dipindahtangankan secara fisik melalui media penyimpanan\n",
    "if not os.path.isdir(f'colorskim_checkpoint/{MODEL[2]}'):\n",
    "        # set random seed\n",
    "        tf.random.set_seed(RANDOM_STATE)\n",
    "        \n",
    "        # Membuat model_2 menggunakan USE\n",
    "        inputs = Input(shape=[], \n",
    "                dtype=tf.string, \n",
    "                name='lapisan_input')\n",
    "        lapisan_embed_pretrained = tf_hub_embedding(inputs)\n",
    "        x = Conv1D(filters=UKURAN_BATCH, \n",
    "                kernel_size=5, \n",
    "                padding='same', \n",
    "                activation='relu',\n",
    "                name='lapisan_konvolusional_1_dimensi')(tf.expand_dims(lapisan_embed_pretrained, axis=-1))\n",
    "        x = GlobalMaxPooling1D(name='lapisan_max_pooling')(x)\n",
    "        outputs = Dense(units=1, \n",
    "                        activation='sigmoid', \n",
    "                        name='lapisan_output')(x)\n",
    "        model_2 = tf.keras.Model(inputs=inputs, \n",
    "                                outputs=outputs, \n",
    "                                name=MODEL[2])\n",
    "\n",
    "        # Compile model_2\n",
    "        model_2.compile(loss=BinaryCrossentropy(),\n",
    "                        optimizer=Adam(),\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "\n",
    "        # Setup wandb init dan config\n",
    "        wb.init(project=wandb['proyek'],\n",
    "                entity=wandb['user'],\n",
    "                name=model_2.name,\n",
    "                config={'epochs': EPOCHS,\n",
    "                        'n_layers': len(model_2.layers)})\n",
    "\n",
    "        # Fit model_2\n",
    "        hist_model_2 = model_2.fit(train_kata_dataset,\n",
    "                                epochs=EPOCHS,\n",
    "                                validation_data=test_kata_dataset,\n",
    "                                callbacks=[wandb_callback(train_kata_dataset),\n",
    "                                           model_checkpoint(model_2.name),\n",
    "                                           reduce_lr_on_plateau(),\n",
    "                                           early_stopping()])\n",
    "        \n",
    "        # tutup logging wandb\n",
    "        wb.finish()\n",
    "else:\n",
    "        # hapus tf_hub_embedding\n",
    "        del tf_hub_embedding\n",
    "        gc.collect()\n",
    "        # load model_2\n",
    "        model_2 = load_model(f'colorskim_checkpoint/{MODEL[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ringkasan dari model_2\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model_2\n",
    "plot_model(model_2, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksplorasi Hasil Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model_2\n",
    "model_2.evaluate(test_kata_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat probabilitas prediksi model_2\n",
    "model_2_pred_prob = tf.squeeze(model_2.predict(test_kata_dataset))\n",
    "model_2_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat prediksi dengan model_2\n",
    "model_2_pred = tf.round(model_2_pred_prob)\n",
    "model_2_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung metriks dari model_2\n",
    "model_2_metrik = hitung_metrik(target=test_target,\n",
    "                               prediksi=model_2_pred)\n",
    "model_2_metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residual dari model_2\n",
    "residual_plot_logr(test_target, model_2, MODEL[2], model_2_metrik['akurasi'], model_2_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix dari model_2\n",
    "plot_conf_matrix(target_label=test_target,\n",
    "                 prediksi_label=model_2_pred,\n",
    "                 nama_model=MODEL[2],\n",
    "                 akurasi=model_2_metrik['akurasi'],\n",
    "                 label_titik_x=['bukan_warna', 'warna'],\n",
    "                 label_titik_y=['bukan_warna', 'warna'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan kesalahan prediksi dalam dataframe\n",
    "df_kesalahan_prediksi(label_encoder=label_encoder,\n",
    "                      test_data=test_data_mnb,\n",
    "                      prediksi=model_2_pred,\n",
    "                      probabilitas_prediksi=model_2_pred_prob,\n",
    "                      order_ulang_header=['brand',\n",
    "                                          'kata',\n",
    "                                          'urut_kata',\n",
    "                                          'total_kata',\n",
    "                                          'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selesai dengan model 2, bersihkan memori di GPU terkait model_2\n",
    "del model_2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Menggunakan Posisi Kata, Tipe Brand dan Lapisan Embed Custom yang diconcatenate\n",
    "\n",
    "Pada skenario training ini, kita akan mencoba untuk tidak hanya menggunakan input data kata dan label saja, tetapi juga menambahkan posisi kata dalam kalimat dan juga tipe brand sebagai variabel independen (fitur) yang mungkin dapat menentukan apakah sebuah kata adalah `bukan_warna` atau `warna`.\n",
    "Pada dasarnya kita akan membuat tumpukan layer yang mempelajari input data per kata dan juga mempelajari posisi dari kata dan juga tipe brand yang kemudian akan ditumpuk (*stack*) menjadi satu lapisan baru."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing untuk `urut_kata` dan `total_kata`?\n",
    "Sebelum kita melakukan *training* pada variabel `urut_kata` dan juga `total_kata`. Terdapat setidaknya dua opsi untuk menggunakan variabel independen ini:\n",
    "\n",
    "1. Melakukan pembagian `urut_kata` dan `total_kata` sehingga nilai dari posisi adalah diantara 0 sampai dengan 1, dimana ~0 menunjukkan posisi kata yang berada dekat dengan awal kalimat dan ~1 adalah posisi kata yang berada dengan akhir kalimat. Namun hal ini akan menyebabkan kita akan kehilangan beberapa informasi dari input data, sebagai contoh nilai 0.5 bisa diartikan sebagai kata pertama dalam kalimat yang terdiri dari dua kata (1/2) atau bisa juga berarti kata kelima dalam kalimat yang terdiri dari sepuluh kata (5/10).\n",
    "2. Menggunakan `urut_kata` dan `total_kata` sebagaimana adanya tanpa melakukan pembagian dan membiarkan model berusaha mempelajari makna dari variabel independen ini.\n",
    "\n",
    "Pada bagian ini, kita akan mencoba untuk menggunakan opsi pertama.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USE atau Custom Embedding?\n",
    "Dikarenakan pada model_2 kita dapat melihat bahwa penggunaan lapisan fitur esktraktor USE tidak dapat dengan baik memprediksi label kata (hal ini bisa disebabkan oleh beberapa hal, seperti fakta bahwa USE dilatih pada dataset *corpus* bahasa internasional yang mungkin lebih baku), maka kita akan kembali menggunakan lapisan *custom embedding* menggunakan Conv1D atau lapisan LSTM dua arah (*bi-directional LSTM*) seperti pada model_1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `urut_kata` dan `total_kata` Embedding\n",
    "\n",
    "Pada bagian ini kita akan melakukan embedding untuk posisional dari sebuah kata dalam kalimat seperti apa yang direpresentasikan oleh kolom `urut_kata` dan `total_kata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribusi jumlah kata dalam artikel - artikel\n",
    "train_data['urut_kata'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['urut_kata'].plot.hist()\n",
    "plt.title('Distribusi Jumlah Kata dalam Artikel', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Karena panjang kata berbeda - beda, maka kita akan menggunakan one_hot encoding\n",
    "# dengan depth max\n",
    "max_kata = int(np.max(train_data['urut_kata']))\n",
    "\n",
    "# Membuat one_hot tensor untuk kolom 'urut_kata'\n",
    "train_data_urut_kata_one_hot = tf.one_hot(train_data['urut_kata'].to_numpy(), \n",
    "                                          depth=max_kata)\n",
    "test_data_urut_kata_one_hot = tf.one_hot(test_data['urut_kata'].to_numpy(),\n",
    "                                         depth=max_kata)\n",
    "train_data_urut_kata_one_hot[:15], train_data_urut_kata_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribusi 'total_kata'\n",
    "train_data['total_kata'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['total_kata'].plot.hist()\n",
    "plt.title('Distribusi Panjang Kalimat dalam Artikel', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_kalimat = int(np.max(train_data['total_kata']))\n",
    "\n",
    "# Membuat one_hot tensor untuk kolom 'total_kata'\n",
    "train_data_total_kata_one_hot = tf.one_hot(train_data['total_kata'].to_numpy(),\n",
    "                                           depth=max_kalimat)\n",
    "test_data_total_kata_one_hot = tf.one_hot(test_data['total_kata'].to_numpy(),\n",
    "                                          depth=max_kalimat)\n",
    "train_data_total_kata_one_hot[:15], train_data_total_kata_one_hot.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Membuat TensorFlow Dataset, Batching dan Preteching untuk Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat train dan test dataset (dengan 4 jenis input data)\n",
    "# Urutan dataset disesuaikan dengan urutan input data pada tf.keras.Model akhir (model_3)\n",
    "train_fitur_data = from_tensor_slices((train_data.iloc[:, 0],\n",
    "                                      train_data.iloc[:, 3:].to_numpy(),\n",
    "                                      train_data_urut_kata_one_hot,\n",
    "                                      train_data_total_kata_one_hot))\n",
    "train_target_data = from_tensor_slices(train_target)\n",
    "train_dataset = zip((train_fitur_data,\n",
    "                     train_target_data))\n",
    "train_dataset = train_dataset.batch(UKURAN_BATCH).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_fitur_data = from_tensor_slices((test_data.iloc[:, 0],\n",
    "                                      test_data.iloc[:, 3:].to_numpy(),\n",
    "                                      test_data_urut_kata_one_hot,\n",
    "                                      test_data_total_kata_one_hot))\n",
    "test_target_data = from_tensor_slices(test_target)\n",
    "test_dataset = zip((test_fitur_data,\n",
    "                    test_target_data))\n",
    "test_dataset = test_dataset.batch(UKURAN_BATCH).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ukuran dan dimensi dataset\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membangun dan Menjalankan Training Model 3\n",
    "\n",
    "1. Membuat model untuk `kata`\n",
    "2. Membuat model untuk `brand`\n",
    "3. Membuat model untuk `urut_kata`\n",
    "4. Membuat model untuk `total_kata`\n",
    "5. Mengkombinasikan output 1 & 2 menggunakan `tf.keras.layers.Concatenate`\n",
    "6. Menambahkan Dense dan Dropout layer untuk poin 5\n",
    "7. Mengkombinasikan output dari poin 3 & 4 menggunakan `tf.keras.layers.Concatenate`\n",
    "8. Mengkombinasikan output 6 & 7 menggunakan `tf.keras.layers.Concatenate`\n",
    "9. Membuat lapisan output yang menerima input dari 2 lapisan embedding di poin 8 dan menghasilkan output probabilitas\n",
    "8. Mengkombinasikan input 1, 2, 3, 4 dan output 9 dalam `tf.keras.Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jika folder dengan path 'colorskim_checkpoint/{model.name}' sudah ada, maka skip fit model \n",
    "# untuk menghemat waktu pengembangan dan hanya load model yang sudah ada dalam folder tersebut \n",
    "# saja\n",
    "if not os.path.isdir(f'colorskim_checkpoint/{MODEL[3]}'):\n",
    "        # set random seed\n",
    "        tf.random.set_seed(RANDOM_STATE)\n",
    "        \n",
    "        # 1. model kata\n",
    "        input_kata = Input(shape=(1,), dtype=tf.string, name='lapisan_input_kata')\n",
    "        lapisan_vektorisasi_kata = lapisan_vektorisasi(input_kata)\n",
    "        lapisan_embedding_kata = lapisan_embedding(lapisan_vektorisasi_kata)\n",
    "        lapisan_bi_lstm_kata = Bidirectional(LSTM(units=UKURAN_BATCH), \n",
    "                                        name='lapisan_bidirectional_lstm_kata')(lapisan_embedding_kata)\n",
    "        # ubah bi_lstm menjadi Conv1D dengan GlobalMaxPooling\n",
    "        # lapisan_Conv1D = Conv1D(filters=UKURAN_BATCH,\n",
    "        #                         kernel_size=5,\n",
    "        #                         padding='same',\n",
    "        #                         activation='relu',\n",
    "        #                         name='lapisan_konvolusional_1D')(lapisan_embedding_kata)\n",
    "        # lapisan_max_pooling = GlobalMaxPooling1D(name='lapisan_max_pooling')(lapisan_Conv1D)\n",
    "        model_kata = Model(input_kata,\n",
    "                        lapisan_bi_lstm_kata,\n",
    "                        name='model_kata')\n",
    "\n",
    "        # 2. model brand\n",
    "        input_brand = Input(shape=(train_data.iloc[:, 3:].shape[1],), \n",
    "                        dtype=tf.float32,\n",
    "                        name='lapisan_input_brand')\n",
    "        x = Dense(units=UKURAN_BATCH, \n",
    "                activation='relu',\n",
    "                name='lapisan_dense_rectified_linear_unit_brand')(input_brand)\n",
    "        model_brand = Model(input_brand,\n",
    "                        x,\n",
    "                        name='model_brand')\n",
    "\n",
    "        # 3. model urut_kata\n",
    "        input_urut_kata = Input(shape=(max_kata,),\n",
    "                                dtype=tf.float32,\n",
    "                                name='lapisan_input_urut_kata')\n",
    "        x = Dense(units=UKURAN_BATCH,\n",
    "                activation='relu',\n",
    "                name='lapisan_dense_rectified_linear_unit_urut_kata')(input_urut_kata)\n",
    "        model_urut_kata = Model(input_urut_kata,\n",
    "                                x,\n",
    "                                name='model_urut_kata')\n",
    "\n",
    "        # 4. model total_kata\n",
    "        input_total_kata = Input(shape=(max_kalimat,),\n",
    "                                dtype=tf.float32,\n",
    "                                name='lapisan_input_total_kata')\n",
    "        x = Dense(units=UKURAN_BATCH,\n",
    "                activation='relu',\n",
    "                name='lapisan_dense_rectified_linear_unit_total_kata')(input_total_kata)\n",
    "        model_total_kata = Model(input_total_kata,\n",
    "                                x,\n",
    "                                name='model_total_kata')\n",
    "\n",
    "        # 5. Mengkombinasikan model kata dan brand\n",
    "        kombinasi_kata_brand = Concatenate(name='lapisan_kombinasi_kata_brand')([model_kata.output, \n",
    "                                                                                model_brand.output])\n",
    "\n",
    "        # 6. Menambahkan lapisan dense dan dropout dari output kombinasi kata dan brand\n",
    "        z = Dense(256, \n",
    "                activation='relu',\n",
    "                name='lapisan_dense_kombinasi_kata_brand')(kombinasi_kata_brand)\n",
    "        z = Dropout(0.5)(z)\n",
    "\n",
    "        # 7. Mengkombinasikan model 3 dan 4\n",
    "        kombinasi_urut_total_kata = Concatenate(name='lapisan_kombinasi_urut_total_kata')([model_urut_kata.output, \n",
    "                                                                                        model_total_kata.output])\n",
    "\n",
    "        # 8. Mengkombinasikan output di poin 6 dengan lapisan_kombinasi_urut_total_kata\n",
    "        kombinasi_kata_brand_urut_total_kata = Concatenate(name='lapisan_kombinasi_kata_brand_urut_total_kata')([z,\n",
    "                                                                                                                kombinasi_urut_total_kata])\n",
    "\n",
    "        # 9. Membuat lapisan output final\n",
    "        lapisan_output = Dense(units=1,\n",
    "                        activation='sigmoid',\n",
    "                        name='lapisan_output_final')(kombinasi_kata_brand_urut_total_kata)\n",
    "\n",
    "        # 10. Menyatukan semua model dari berbagai input\n",
    "        model_3 = Model(inputs=[model_kata.input,\n",
    "                                model_brand.input,\n",
    "                                model_urut_kata.input,\n",
    "                                model_total_kata.input],\n",
    "                        outputs=lapisan_output,\n",
    "                        name=MODEL[3])\n",
    "\n",
    "        # Compile model_3\n",
    "        model_3.compile(loss=BinaryCrossentropy(),\n",
    "                        optimizer=Adam(),\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "        # Setup wandb init dan config\n",
    "        wb.init(project=wandb['proyek'],\n",
    "                entity=wandb['user'],\n",
    "                name=model_3.name,\n",
    "                config={'epochs': EPOCHS,\n",
    "                        'n_layers': len(model_3.layers)})\n",
    "\n",
    "\n",
    "        # Fit model_3\n",
    "        hist_model_3 = model_3.fit(train_dataset,\n",
    "                                epochs=EPOCHS,\n",
    "                                validation_data=test_dataset,\n",
    "                                callbacks=[wandb_callback(train_dataset),\n",
    "                                        model_checkpoint(model_3.name),\n",
    "                                        reduce_lr_on_plateau(),\n",
    "                                        early_stopping()])\n",
    "        \n",
    "        # tutup logging wandb\n",
    "        wb.finish()\n",
    "else:\n",
    "        # load model_3\n",
    "        model_3 = load_model(f'colorskim_checkpoint/{MODEL[3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ringkasan dari model_3\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot struktur model_3\n",
    "plot_model(model_3, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksplorasi Hasil Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluasi model_3\n",
    "model_3.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediksi probabilitas model_3\n",
    "model_3_pred_prob = model_3.predict(test_dataset)\n",
    "model_3_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat prediksi model_3\n",
    "model_3_pred = tf.squeeze(tf.round(model_3_pred_prob))\n",
    "model_3_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metriks skor model_3\n",
    "model_3_metrik = hitung_metrik(target=test_target,\n",
    "                               prediksi=model_3_pred)\n",
    "model_3_metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residual model_3\n",
    "residual_plot_logr(test_target=test_target,\n",
    "                   model=model_3,\n",
    "                   nama_model=MODEL[3],\n",
    "                   model_akurasi=model_3_metrik['akurasi'],\n",
    "                   probabilitas_prediksi_model=tf.squeeze(model_3_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "plot_conf_matrix(target_label=test_target,\n",
    "                 prediksi_label=model_3_pred,\n",
    "                 nama_model=MODEL[3],\n",
    "                 akurasi=model_3_metrik['akurasi'],\n",
    "                 label_titik_x=['bukan_warna', 'warna'],\n",
    "                 label_titik_y=['bukan_warna', 'warna'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe kesalahan prediksi model_3\n",
    "df_kesalahan_prediksi(label_encoder=label_encoder,\n",
    "                      test_data=test_data_mnb,\n",
    "                      prediksi=model_3_pred,\n",
    "                      probabilitas_prediksi=model_3_pred_prob,\n",
    "                      order_ulang_header=['brand',\n",
    "                                          'kata',\n",
    "                                          'urut_kata',\n",
    "                                          'total_kata',\n",
    "                                          'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.get_memory_info('GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf-py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79d93ff88d3400081c5bbc856760c635900de0c16bb50c43f47156b09020e5ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
