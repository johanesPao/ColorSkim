{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColorSkim Machine Learning AI\n",
    "\n",
    "Saat ini `item_description` untuk artikel ditulis dalam bentuk/format `nama_artikel + warna` dimana pemisahan `nama_artikel` dan `warna` bervariasi antar brand, beberapa menggunakan spasi, dash, garis miring dsbnya.\n",
    "\n",
    "Pembelajaran mesin ini merupakan pembelajaran yang akan menerapkan jaringan saraf buatan (neural network) untuk mempelajari pola penulisan artikel yang bercampur dengan warna untuk mengekstrak warna saja dari artikel.\n",
    "\n",
    "Akan dilakukan beberapa scenario modelling **Natural Language Procesing** untuk permasalahan *sequence to sequence* ini. Pada intinya kita akan membagi kalimat (`item_description`) berdasarkan kata per kata dan mengkategorisasikan masing - masing kata ke dalam satu dari dua kategori warna atau bukan_warna (logistik biner).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modul\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb as wb\n",
    "from rahasia import API_KEY_WANDB\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"/device:GPU:0\"\n",
       "device_type: \"GPU\"\n",
       "memory_limit: 1406005863\n",
       "locality {\n",
       "  bus_id: 1\n",
       "  links {\n",
       "  }\n",
       "}\n",
       "incarnation: 909604535619376993\n",
       "physical_device_desc: \"device: 0, name: NVIDIA GeForce MX250, pci bus id: 0000:02:00.0, compute capability: 6.1\"\n",
       "xla_global_id: 416903419"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cek ketersediaan GPU untuk modeling\n",
    "# GeForce MX250 - office\n",
    "# GeForce GTX 1060 - home\n",
    "device_lib.list_local_devices()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\jPao/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# login ke wandb\n",
    "wb.login(key=API_KEY_WANDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membaca data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nama_artikel</th>\n",
       "      <th>kata</th>\n",
       "      <th>label</th>\n",
       "      <th>urut_kata</th>\n",
       "      <th>total_kata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADISSAGE-BLACK/BLACK/RUNWHT</td>\n",
       "      <td>ADISSAGE</td>\n",
       "      <td>bukan_warna</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADISSAGE-BLACK/BLACK/RUNWHT</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>warna</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADISSAGE-BLACK/BLACK/RUNWHT</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>warna</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADISSAGE-BLACK/BLACK/RUNWHT</td>\n",
       "      <td>RUNWHT</td>\n",
       "      <td>warna</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADISSAGE-N.NAVY/N.NAVY/RUNWHT</td>\n",
       "      <td>ADISSAGE</td>\n",
       "      <td>bukan_warna</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ADISSAGE-N.NAVY/N.NAVY/RUNWHT</td>\n",
       "      <td>N.NAVY</td>\n",
       "      <td>warna</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ADISSAGE-N.NAVY/N.NAVY/RUNWHT</td>\n",
       "      <td>N.NAVY</td>\n",
       "      <td>warna</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ADISSAGE-N.NAVY/N.NAVY/RUNWHT</td>\n",
       "      <td>RUNWHT</td>\n",
       "      <td>warna</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3 STRIPE D 29.5-BASKETBALL NATURAL</td>\n",
       "      <td>3</td>\n",
       "      <td>bukan_warna</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3 STRIPE D 29.5-BASKETBALL NATURAL</td>\n",
       "      <td>STRIPE</td>\n",
       "      <td>bukan_warna</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         nama_artikel      kata        label  urut_kata  \\\n",
       "0         ADISSAGE-BLACK/BLACK/RUNWHT  ADISSAGE  bukan_warna          1   \n",
       "1         ADISSAGE-BLACK/BLACK/RUNWHT     BLACK        warna          2   \n",
       "2         ADISSAGE-BLACK/BLACK/RUNWHT     BLACK        warna          3   \n",
       "3         ADISSAGE-BLACK/BLACK/RUNWHT    RUNWHT        warna          4   \n",
       "4       ADISSAGE-N.NAVY/N.NAVY/RUNWHT  ADISSAGE  bukan_warna          1   \n",
       "5       ADISSAGE-N.NAVY/N.NAVY/RUNWHT    N.NAVY        warna          2   \n",
       "6       ADISSAGE-N.NAVY/N.NAVY/RUNWHT    N.NAVY        warna          3   \n",
       "7       ADISSAGE-N.NAVY/N.NAVY/RUNWHT    RUNWHT        warna          4   \n",
       "8  3 STRIPE D 29.5-BASKETBALL NATURAL         3  bukan_warna          1   \n",
       "9  3 STRIPE D 29.5-BASKETBALL NATURAL    STRIPE  bukan_warna          2   \n",
       "\n",
       "   total_kata  \n",
       "0           4  \n",
       "1           4  \n",
       "2           4  \n",
       "3           4  \n",
       "4           4  \n",
       "5           4  \n",
       "6           4  \n",
       "7           4  \n",
       "8           6  \n",
       "9           6  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Membaca data ke dalam DataFrame pandas\n",
    "data = pd.read_csv('data/setengah_dataset_artikel.csv')\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksplorasi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bukan_warna    34174\n",
       "warna          22577\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribusi label dalam data\n",
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konversi data ke dalam train dan test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['INVIS', 'SOLRED', 'JR', 'REACT', 'WHITE'], dtype=object),\n",
       " array(['6', 'GA', 'NIKE', 'BLUE', 'BLACK'], dtype=object),\n",
       " array(['bukan_warna', 'warna', 'bukan_warna', 'bukan_warna', 'warna'],\n",
       "       dtype=object),\n",
       " array(['bukan_warna', 'bukan_warna', 'bukan_warna', 'warna', 'warna'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_kata, test_kata, train_label, test_label = train_test_split(data['kata'].to_numpy(), data['label'].to_numpy(), test_size=0.2, random_state=42)\n",
    "train_kata[:5], test_kata[:5], train_label[:5], test_label[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konversi label ke dalam numerik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 0, 1]), array([0, 0, 0, 1, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_label_encode = label_encoder.fit_transform(train_label)\n",
    "test_label_encode = label_encoder.transform(test_label)\n",
    "train_label_encode[:5], test_label_encode[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 0: model dasar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tf-idf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tf-idf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tf-idf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Membuat pipeline untuk mengubah kata ke dalam tf-idf\n",
    "model_0 = Pipeline([\n",
    "    (\"tf-idf\", TfidfVectorizer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit pipeline dengan data training\n",
    "model_0.fit(X=train_kata, y=train_label_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9935688485595983"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluasi model_0 pada data test\n",
    "model_0.score(X=test_kata, y=test_label_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Membuat prediksi menggunakan data test\n",
    "pred_model_0 = model_0.predict(test_kata)\n",
    "pred_model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat fungsi dasar untuk menghitung accuray, precision, recall, f1-score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "def hitung_metrik(target, prediksi):\n",
    "    \"\"\"\n",
    "    Menghitung accuracy, precision, recall dan f1-score dari model klasifikasi biner\n",
    "    \n",
    "    Args:\n",
    "        target: label yang sebenarnya dalam bentuk 1D array\n",
    "        prediksi: label yang diprediksi dalam bentuk 1D array\n",
    "        \n",
    "    Returns:\n",
    "        nilai accuracy, precision, recall dan f1-score dalam bentuk dictionary\n",
    "    \"\"\"\n",
    "    # Menghitung akurasi model\n",
    "    model_akurasi = accuracy_score(target, prediksi)\n",
    "    # Menghitung precision, recall, f1-score dan support dari model\n",
    "    model_presisi, model_recall, model_f1, _ = precision_recall_fscore_support(target, prediksi, average='weighted')\n",
    "    \n",
    "    hasil_model = {'akurasi': model_akurasi,\n",
    "                   'presisi': model_presisi,\n",
    "                   'recall': model_recall,\n",
    "                   'f1-score': model_f1}\n",
    "    \n",
    "    return hasil_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'akurasi': 0.9935688485595983,\n",
       " 'presisi': 0.9935690437085363,\n",
       " 'recall': 0.9935688485595983,\n",
       " 'f1-score': 0.9935671438217326}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menghitung metrik dari model_0\n",
    "model_0_metrik = hitung_metrik(target=test_label_encode, \n",
    "                               prediksi=pred_model_0)\n",
    "model_0_metrik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menyiapkan data (text) untuk model deep sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Vectorizer Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45400"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jumlah data (kata) dalam train_data\n",
    "len(train_kata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2940"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jumlah data unik (kata unik) dalam train_kata\n",
    "jumlah_kata_train = len(np.unique(train_kata))\n",
    "jumlah_kata_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat text vectorizer\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "vectorizer_kata = TextVectorization(max_tokens=jumlah_kata_train,\n",
    "                                    output_sequence_length=1,\n",
    "                                    standardize='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengadaptaasikan text vectorizer ke dalam train_kata\n",
    "vectorizer_kata.adapt(train_kata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kata:\n",
      "1PP\n",
      "\n",
      "Kata setelah vektorisasi:\n",
      "[[364]]\n"
     ]
    }
   ],
   "source": [
    "# Test vectorizer kata\n",
    "import random\n",
    "target_kata = random.choice(train_kata)\n",
    "print(f'Kata:\\n{target_kata}\\n')\n",
    "print(f'Kata setelah vektorisasi:\\n{vectorizer_kata([target_kata])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'text_vectorization',\n",
       " 'trainable': True,\n",
       " 'batch_input_shape': (None,),\n",
       " 'dtype': 'string',\n",
       " 'max_tokens': 2940,\n",
       " 'standardize': 'lower',\n",
       " 'split': 'whitespace',\n",
       " 'ngrams': None,\n",
       " 'output_mode': 'int',\n",
       " 'output_sequence_length': 1,\n",
       " 'pad_to_max_tokens': False,\n",
       " 'sparse': False,\n",
       " 'ragged': False,\n",
       " 'vocabulary': None,\n",
       " 'idf_weights': None}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_kata.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2938"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jumlah vocabulary dalam vectorizer_kata\n",
    "jumlah_vocab = vectorizer_kata.get_vocabulary()\n",
    "len(jumlah_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membuat Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat text embedding layer\n",
    "from tensorflow.keras.layers import Embedding\n",
    "kata_embed = Embedding(input_dim=len(jumlah_vocab),\n",
    "                       output_dim=64,\n",
    "                       mask_zero=True,\n",
    "                       name='layer_token_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kata sebelum vektorisasi:\n",
      "1PP\n",
      "\n",
      "\n",
      "Kata sesudah vektorisasi (sebelum embedding):\n",
      "[[364]]\n",
      "\n",
      "\n",
      "Kata setelah embedding:\n",
      "[[[-4.68876362e-02  1.39226429e-02  1.53589956e-02  4.58656624e-03\n",
      "    3.38894464e-02 -6.04242086e-06  2.18849666e-02  1.71338394e-03\n",
      "   -4.49796915e-02 -8.10725614e-03 -5.08914143e-03 -4.80378158e-02\n",
      "   -2.03994270e-02  4.37932089e-03  1.24381408e-02 -1.91058517e-02\n",
      "    1.82107799e-02  4.98666205e-02 -3.25337872e-02  1.53612345e-04\n",
      "   -3.75577807e-02  1.64883621e-02 -4.04143445e-02 -3.25289257e-02\n",
      "    4.32353057e-02  2.53272094e-02  2.05454491e-02  2.43189670e-02\n",
      "    4.46835868e-02  4.18224074e-02 -1.92648061e-02 -6.11367077e-03\n",
      "    2.24282406e-02  3.76079567e-02 -3.73811647e-03 -4.47501801e-02\n",
      "   -3.07079442e-02  2.99662985e-02  3.80241610e-02 -2.02864297e-02\n",
      "   -2.23221537e-02  1.91515349e-02  2.69093402e-02  1.99612118e-02\n",
      "    3.74258496e-02  1.38394535e-05  1.75894536e-02 -3.01381350e-02\n",
      "    3.93041410e-02 -2.25336794e-02  3.17663290e-02 -3.27903517e-02\n",
      "    2.77244337e-02  2.51912810e-02  1.21850856e-02 -2.95726657e-02\n",
      "   -1.71264187e-02  3.92391421e-02  1.28514208e-02 -2.36031897e-02\n",
      "   -3.78655195e-02  4.99851964e-02  3.94029133e-02  2.12919377e-02]]]\n",
      "\n",
      "Shape dari kata setelah embedding:\n",
      "(1, 1, 64)\n"
     ]
    }
   ],
   "source": [
    "# Contoh vectorizer dan embedding\n",
    "print(f'Kata sebelum vektorisasi:\\n{target_kata}\\n')\n",
    "kata_tervektor = vectorizer_kata([target_kata])\n",
    "print(f'\\nKata sesudah vektorisasi (sebelum embedding):\\n{kata_tervektor}\\n')\n",
    "kata_terembed = kata_embed(kata_tervektor)\n",
    "print(f'\\nKata setelah embedding:\\n{kata_terembed}\\n')\n",
    "print(f'Shape dari kata setelah embedding:\\n{kata_terembed.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membuat TensorFlow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Membuat TensorFlow dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_kata, train_label_encode))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_kata, test_label_encode))\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Membuat TensorSliceDataset menjadi prefetched dataset\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Conv1D dengan embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat model_1 dengan layer Conv1D dari kata yang divektorisasi dan di-embed\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string, name='layer_input')\n",
    "layer_vektor = vectorizer_kata(inputs)\n",
    "layer_embed = kata_embed(layer_vektor)\n",
    "x = layers.Conv1D(filters=64, kernel_size=5, padding='same', activation='relu')(layer_embed)\n",
    "x = layers.GlobalMaxPooling1D(name='layer_max_pool')(x)\n",
    "outputs = layers.Dense(units=1, activation='sigmoid', name='layer_output')(x)\n",
    "model_1 = tf.keras.Model(inputs=inputs, outputs=outputs, name='model_1_Conv1D_embed')\n",
    "\n",
    "# Compile\n",
    "model_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_Conv1D_embed\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer_input (InputLayer)    [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 1)                0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " layer_token_embedding (Embe  (None, 1, 64)            188032    \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 1, 64)             20544     \n",
      "                                                                 \n",
      " layer_max_pool (GlobalMaxPo  (None, 64)               0         \n",
      " oling1D)                                                        \n",
      "                                                                 \n",
      " layer_output (Dense)        (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 208,641\n",
      "Trainable params: 208,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Ringkasa model_1\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "# Plot model_1\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model_1, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2cl8n813) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>GFLOPS</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model_1_Conv1D_embed</strong>: <a href=\"https://wandb.ai/jpao/ColorSkim/runs/2cl8n813\" target=\"_blank\">https://wandb.ai/jpao/ColorSkim/runs/2cl8n813</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220706_175253-2cl8n813\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2cl8n813). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\ColorSkim\\wandb\\run-20220706_175519-1w82lbhy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jpao/ColorSkim/runs/1w82lbhy\" target=\"_blank\">model_1_Conv1D_embed</a></strong> to <a href=\"https://wandb.ai/jpao/ColorSkim\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1419/1419 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9823"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as W&B Artifacts in the SavedModel format.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220706_175519-1w82lbhy\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220706_175519-1w82lbhy\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ColorSkim\\wandb\\run-20220706_175519-1w82lbhy\\files\\model-best)... Done. 0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1419/1419 [==============================] - 130s 91ms/step - loss: 0.0726 - accuracy: 0.9823 - val_loss: 0.0277 - val_accuracy: 0.9934 - _timestamp: 1657105053.0000 - _runtime: 127.0000\n",
      "Epoch 2/5\n",
      "1419/1419 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9951"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220706_175519-1w82lbhy\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220706_175519-1w82lbhy\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ColorSkim\\wandb\\run-20220706_175519-1w82lbhy\\files\\model-best)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1419/1419 [==============================] - 148s 104ms/step - loss: 0.0181 - accuracy: 0.9951 - val_loss: 0.0266 - val_accuracy: 0.9931 - _timestamp: 1657105205.0000 - _runtime: 279.0000\n",
      "Epoch 3/5\n",
      "1419/1419 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9956"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220706_175519-1w82lbhy\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220706_175519-1w82lbhy\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ColorSkim\\wandb\\run-20220706_175519-1w82lbhy\\files\\model-best)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1419/1419 [==============================] - 104s 73ms/step - loss: 0.0161 - accuracy: 0.9956 - val_loss: 0.0259 - val_accuracy: 0.9931 - _timestamp: 1657105311.0000 - _runtime: 385.0000\n",
      "Epoch 4/5\n",
      "1419/1419 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9957"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220706_175519-1w82lbhy\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220706_175519-1w82lbhy\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ColorSkim\\wandb\\run-20220706_175519-1w82lbhy\\files\\model-best)... Done. 0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1419/1419 [==============================] - 112s 79ms/step - loss: 0.0152 - accuracy: 0.9957 - val_loss: 0.0257 - val_accuracy: 0.9937 - _timestamp: 1657105422.0000 - _runtime: 496.0000\n",
      "Epoch 5/5\n",
      "1419/1419 [==============================] - 117s 82ms/step - loss: 0.0146 - accuracy: 0.9958 - val_loss: 0.0257 - val_accuracy: 0.9937 - _timestamp: 1657105544.0000 - _runtime: 618.0000\n"
     ]
    }
   ],
   "source": [
    "# import WandbCallback\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# Setup wandb init dan config\n",
    "wb.init(project='ColorSkim',\n",
    "        entity='jpao',\n",
    "        name='model_1_Conv1D_embed',\n",
    "        config={'epochs': 5,\n",
    "                'n_layers': len(model_1.layers)})\n",
    "\n",
    "# Fit model_1\n",
    "hist_model_1 = model_1.fit(train_dataset,\n",
    "                           epochs=wb.config.epochs,\n",
    "                           validation_data=test_dataset,\n",
    "                           callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\ColorSkim\\ColorSkim_AI.ipynb Cell 39\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ColorSkim/ColorSkim_AI.ipynb#ch0000041?line=4'>5</a>\u001b[0m model_test \u001b[39m=\u001b[39m model_1\u001b[39m.\u001b[39mpredict(article\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msplit())\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ColorSkim/ColorSkim_AI.ipynb#ch0000041?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(article_list)):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/ColorSkim/ColorSkim_AI.ipynb#ch0000041?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mKata: \u001b[39m\u001b[39m{\u001b[39;00marticle_list[i]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mPrediksi: \u001b[39m\u001b[39m{\u001b[39;00mclass_list[model_test[i]]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# Test prediksi dengan model_1 (model_1_Conv1D_embed)\n",
    "class_list = ['warna', 'bukan_warna']\n",
    "article = 'PUMA XTG WOVEN PANTS PUMA BLACK-PUMA WHITE'\n",
    "article_list = article.replace(\"-\",\" \").split()\n",
    "model_test = model_1.predict(article.replace(\"-\",\" \").split())\n",
    "for i in range(0, len(article_list)):\n",
    "    print(f'Kata: {article_list[i]}\\nPrediksi: {class_list[model_test[i]]}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf-py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79d93ff88d3400081c5bbc856760c635900de0c16bb50c43f47156b09020e5ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
