{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ColorSkim AI Machine Learning The voice that navigated was definitely that of a machine, and yet you could tell that the machine was a woman, which hurt my mind a little. How can machines have genders? The machine also had an American accent. How can machines have nationalities? This can't be a good idea, making machines talk like real people, can it? Giving machines humanoid identities? - Matthew Quick, The Good Luck of Right Now Latar Belakang Natural Language Processing Isi latar belakang disini.. Objektif Isi objektif disini","title":"Permasalahan dan Objektif"},{"location":"#colorskim-ai-machine-learning","text":"The voice that navigated was definitely that of a machine, and yet you could tell that the machine was a woman, which hurt my mind a little. How can machines have genders? The machine also had an American accent. How can machines have nationalities? This can't be a good idea, making machines talk like real people, can it? Giving machines humanoid identities? - Matthew Quick, The Good Luck of Right Now","title":"ColorSkim AI Machine Learning"},{"location":"#latar-belakang-natural-language-processing","text":"Isi latar belakang disini..","title":"Latar Belakang Natural Language Processing"},{"location":"#objektif","text":"Isi objektif disini","title":"Objektif"},{"location":"ColorSkim_AI/","text":"ColorSkim Machine Learning AI Saat ini item_description untuk artikel ditulis dalam bentuk/format nama_artikel + warna dimana pemisahan nama_artikel dan warna bervariasi antar brand, beberapa menggunakan spasi, dash, garis miring dsbnya. Pembelajaran mesin ini merupakan pembelajaran yang akan menerapkan jaringan saraf buatan (neural network) untuk mempelajari pola penulisan artikel yang bercampur dengan warna untuk mengekstrak warna saja dari artikel. Akan dilakukan beberapa scenario modelling Natural Language Procesing untuk permasalahan sequence to sequence ini. Pada intinya kita akan membagi kalimat ( item_description ) berdasarkan kata per kata dan mengkategorisasikan masing - masing kata ke dalam satu dari dua kategori warna atau bukan_warna (logistik biner). 1 2 3 4 5 6 7 8 9 10 # import modul import tensorflow as tf from tensorflow.python.client import device_lib import pandas as pd import numpy as np import matplotlib.pyplot as plt import wandb as wb from rahasia import API_KEY_WANDB tf . config . run_functions_eagerly ( True ) tf . data . experimental . enable_debug_mode () 1 2 3 4 # cek ketersediaan GPU untuk modeling # NVidia GeForce MX250 - office # NVidia GeForce GTX1060 - home device_lib . list_local_devices ()[ 1 ] 1 2 3 4 5 6 7 8 9 10 11 name: \"/device:GPU:0\" device_type: \"GPU\" memory_limit: 4852809728 locality { bus_id: 1 links { } } incarnation: 17804305938031545614 physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\" xla_global_id: 416903419 1 2 # login ke wandb wb . login ( key = API_KEY_WANDB ) 1 2 3 4 5 6 7 8 9 10 11 Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving. \u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjpao\u001b[0m (\u001b[33mpri-data\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly. \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line. \u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\jPao/.netrc True Membaca data 1 2 3 # Membaca data ke dalam DataFrame pandas data = pd . read_csv ( 'data/setengah_dataset_artikel.csv' ) data [: 10 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } nama_artikel kata label urut_kata total_kata 0 ADISSAGE-BLACK/BLACK/RUNWHT ADISSAGE bukan_warna 1 4 1 ADISSAGE-BLACK/BLACK/RUNWHT BLACK warna 2 4 2 ADISSAGE-BLACK/BLACK/RUNWHT BLACK warna 3 4 3 ADISSAGE-BLACK/BLACK/RUNWHT RUNWHT warna 4 4 4 ADISSAGE-N.NAVY/N.NAVY/RUNWHT ADISSAGE bukan_warna 1 4 5 ADISSAGE-N.NAVY/N.NAVY/RUNWHT N.NAVY warna 2 4 6 ADISSAGE-N.NAVY/N.NAVY/RUNWHT N.NAVY warna 3 4 7 ADISSAGE-N.NAVY/N.NAVY/RUNWHT RUNWHT warna 4 4 8 3 STRIPE D 29.5-BASKETBALL NATURAL 3 bukan_warna 1 6 9 3 STRIPE D 29.5-BASKETBALL NATURAL STRIPE bukan_warna 2 6 Eksplorasi data 1 2 # distribusi label dalam data data [ 'label' ] . value_counts () 1 2 3 bukan_warna 34174 warna 22577 Name: label, dtype: int64 Konversi data ke dalam train dan test 1 2 3 from sklearn.model_selection import train_test_split train_kata , test_kata , train_label , test_label = train_test_split ( data [ 'kata' ] . to_numpy (), data [ 'label' ] . to_numpy (), test_size = 0.2 , random_state = 42 ) train_kata [: 5 ], test_kata [: 5 ], train_label [: 5 ], test_label [: 5 ] 1 2 3 4 5 6 (array(['INVIS', 'SOLRED', 'JR', 'REACT', 'WHITE'], dtype=object), array(['6', 'GA', 'NIKE', 'BLUE', 'BLACK'], dtype=object), array(['bukan_warna', 'warna', 'bukan_warna', 'bukan_warna', 'warna'], dtype=object), array(['bukan_warna', 'bukan_warna', 'bukan_warna', 'warna', 'warna'], dtype=object)) Konversi label ke dalam numerik 1 2 3 4 5 from sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder () train_label_encode = label_encoder . fit_transform ( train_label ) test_label_encode = label_encoder . transform ( test_label ) train_label_encode [: 5 ], test_label_encode [: 5 ] 1 (array([0, 1, 0, 0, 1]), array([0, 0, 0, 1, 1])) Model 0: model dasar 1 2 3 4 5 6 7 8 9 10 11 12 from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline # Membuat pipeline untuk mengubah kata ke dalam tf-idf model_0 = Pipeline ([ ( \"tf-idf\" , TfidfVectorizer ()), ( \"clf\" , MultinomialNB ()) ]) # Fit pipeline dengan data training model_0 . fit ( X = train_kata , y = train_label_encode ) #sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\u25b8\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\u25be\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;} Pipeline(steps=[('tf-idf', TfidfVectorizer()), ('clf', MultinomialNB())]) In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. Pipeline Pipeline(steps=[('tf-idf', TfidfVectorizer()), ('clf', MultinomialNB())]) TfidfVectorizer TfidfVectorizer() MultinomialNB MultinomialNB() 1 2 # Evaluasi model_0 pada data test model_0 . score ( X = test_kata , y = test_label_encode ) 1 0.9935688485595983 1 2 3 # Membuat prediksi menggunakan data test pred_model_0 = model_0 . predict ( test_kata ) pred_model_0 1 array([0, 0, 0, ..., 1, 1, 0]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # Membuat fungsi dasar untuk menghitung accuray, precision, recall, f1-score from sklearn.metrics import accuracy_score , precision_recall_fscore_support def hitung_metrik ( target , prediksi ): \"\"\" Menghitung accuracy, precision, recall dan f1-score dari model klasifikasi biner Args: target: label yang sebenarnya dalam bentuk 1D array prediksi: label yang diprediksi dalam bentuk 1D array Returns: nilai accuracy, precision, recall dan f1-score dalam bentuk dictionary \"\"\" # Menghitung akurasi model model_akurasi = accuracy_score ( target , prediksi ) # Menghitung precision, recall, f1-score dan support dari model model_presisi , model_recall , model_f1 , _ = precision_recall_fscore_support ( target , prediksi , average = 'weighted' ) hasil_model = { 'akurasi' : model_akurasi , 'presisi' : model_presisi , 'recall' : model_recall , 'f1-score' : model_f1 } return hasil_model 1 2 3 4 # Menghitung metrik dari model_0 model_0_metrik = hitung_metrik ( target = test_label_encode , prediksi = pred_model_0 ) model_0_metrik 1 2 3 4 {'akurasi': 0.9935688485595983, 'presisi': 0.9935690437085363, 'recall': 0.9935688485595983, 'f1-score': 0.9935671438217326} Menyiapkan data (text) untuk model deep sequence Text Vectorizer Layer 1 2 # jumlah data (kata) dalam train_data len ( train_kata ) 1 45400 1 2 3 # jumlah data unik (kata unik) dalam train_kata jumlah_kata_train = len ( np . unique ( train_kata )) jumlah_kata_train 1 2940 1 2 3 4 5 # Membuat text vectorizer from tensorflow.keras.layers import TextVectorization vectorizer_kata = TextVectorization ( max_tokens = jumlah_kata_train , output_sequence_length = 1 , standardize = 'lower' ) 1 2 # Mengadaptaasikan text vectorizer ke dalam train_kata vectorizer_kata . adapt ( train_kata ) 1 2 3 4 5 # Test vectorizer kata import random target_kata = random . choice ( train_kata ) print ( f 'Kata: \\n { target_kata } \\n ' ) print ( f 'Kata setelah vektorisasi: \\n { vectorizer_kata ([ target_kata ]) } ' ) 1 2 3 4 5 Kata: 5 Kata setelah vektorisasi: [[47]] 1 vectorizer_kata . get_config () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 {'name': 'text_vectorization', 'trainable': True, 'batch_input_shape': (None,), 'dtype': 'string', 'max_tokens': 2940, 'standardize': 'lower', 'split': 'whitespace', 'ngrams': None, 'output_mode': 'int', 'output_sequence_length': 1, 'pad_to_max_tokens': False, 'sparse': False, 'ragged': False, 'vocabulary': None, 'idf_weights': None} 1 2 3 # Jumlah vocabulary dalam vectorizer_kata jumlah_vocab = vectorizer_kata . get_vocabulary () len ( jumlah_vocab ) 1 2938 Membuat Text Embedding 1 2 3 4 5 6 # Membuat text embedding layer from tensorflow.keras.layers import Embedding kata_embed = Embedding ( input_dim = len ( jumlah_vocab ), output_dim = 64 , mask_zero = True , name = 'layer_token_embedding' ) 1 2 3 4 5 6 7 # Contoh vectorizer dan embedding print ( f 'Kata sebelum vektorisasi: \\n { target_kata } \\n ' ) kata_tervektor = vectorizer_kata ([ target_kata ]) print ( f ' \\n Kata sesudah vektorisasi (sebelum embedding): \\n { kata_tervektor } \\n ' ) kata_terembed = kata_embed ( kata_tervektor ) print ( f ' \\n Kata setelah embedding: \\n { kata_terembed } \\n ' ) print ( f 'Shape dari kata setelah embedding: \\n { kata_terembed . shape } ' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Kata sebelum vektorisasi: 5 Kata sesudah vektorisasi (sebelum embedding): [[47]] Kata setelah embedding: [[[-0.00258959 0.04990998 -0.04441559 -0.02205954 -0.03956609 -0.01799051 -0.02189567 -0.01263437 0.04183039 -0.029005 -0.02622869 0.02765231 -0.04219884 0.00494075 -0.04544568 -0.02989398 0.0400069 -0.03978921 0.03846495 0.01232683 0.0007933 -0.02543398 0.00365927 -0.01668191 0.01294837 0.00481149 -0.02894466 -0.02507013 0.04588108 0.0405197 -0.01860956 -0.04605418 -0.03735595 -0.00483001 -0.00458407 -0.01903701 0.03657718 0.00718967 0.01976332 -0.02251861 0.00589142 -0.00238026 -0.01294807 0.02448275 -0.02560136 0.033471 0.0314525 0.00829456 -0.03963711 0.03865555 0.01974393 0.00157628 -0.01058214 0.04987845 -0.01076373 -0.02952557 0.03197506 -0.01180102 -0.02673483 0.03148599 -0.04402238 0.01858581 -0.01718934 -0.00017769]]] Shape dari kata setelah embedding: (1, 1, 64) Membuat TensorFlow Dataset 1 2 3 4 5 # Membuat TensorFlow dataset train_dataset = tf . data . Dataset . from_tensor_slices (( train_kata , train_label_encode )) test_dataset = tf . data . Dataset . from_tensor_slices (( test_kata , test_label_encode )) train_dataset 1 <TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))> 1 2 3 4 5 # Membuat TensorSliceDataset menjadi prefetched dataset train_dataset = train_dataset . batch ( 32 ) . prefetch ( tf . data . AUTOTUNE ) test_dataset = test_dataset . batch ( 32 ) . prefetch ( tf . data . AUTOTUNE ) train_dataset 1 <BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))> Model 1: Conv1D dengan embedding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Membuat model_1 dengan layer Conv1D dari kata yang divektorisasi dan di-embed from tensorflow.keras import layers inputs = layers . Input ( shape = ( 1 ,), dtype = tf . string , name = 'layer_input' ) layer_vektor = vectorizer_kata ( inputs ) layer_embed = kata_embed ( layer_vektor ) x = layers . Conv1D ( filters = 64 , kernel_size = 5 , padding = 'same' , activation = 'relu' )( layer_embed ) x = layers . GlobalMaxPooling1D ( name = 'layer_max_pool' )( x ) outputs = layers . Dense ( units = 1 , activation = 'sigmoid' , name = 'layer_output' )( x ) model_1 = tf . keras . Model ( inputs = inputs , outputs = outputs , name = 'model_1_Conv1D_embed' ) # Compile model_1 . compile ( loss = tf . keras . losses . BinaryCrossentropy (), optimizer = tf . keras . optimizers . Adam (), metrics = [ 'accuracy' ]) 1 2 # Ringkasa model_1 model_1 . summary () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Model: \"model_1_Conv1D_embed\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= layer_input (InputLayer) [(None, 1)] 0 text_vectorization (TextVec (None, 1) 0 torization) layer_token_embedding (Embe (None, 1, 64) 188032 dding) conv1d (Conv1D) (None, 1, 64) 20544 layer_max_pool (GlobalMaxPo (None, 64) 0 oling1D) layer_output (Dense) (None, 1) 65 ================================================================= Total params: 208,641 Trainable params: 208,641 Non-trainable params: 0 _________________________________________________________________ 1 2 3 # Plot model_1 from tensorflow.keras.utils import plot_model plot_model ( model_1 , show_shapes = True ) 1 You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # import WandbCallback from wandb.keras import WandbCallback # Setup wandb init dan config wb . init ( project = 'ColorSkim' , entity = 'jpao' , name = 'model_1_Conv1D_embed' , config = { 'epochs' : 3 , 'n_layers' : len ( model_1 . layers )}) # Fit model_1 hist_model_1 = model_1 . fit ( train_dataset , epochs = wb . config . epochs , validation_data = test_dataset , callbacks = [ WandbCallback ()]) 1 \u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjpao\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin Tracking run with wandb version 0.12.21 Run data is saved locally in d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s Syncing run model_1_Conv1D_embed to Weights & Biases ( docs ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts. WARNING:tensorflow:Issue encountered when serializing table_initializer. Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore. 'NoneType' object has no attribute 'name' WARNING:tensorflow:From c:\\Users\\jPao\\miniconda3\\envs\\tf-py39\\lib\\site-packages\\tensorflow\\python\\profiler\\internal\\flops_registry.py:138: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name` Epoch 1/3 1419/1419 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9841 \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as W&B Artifacts in the SavedModel format. WARNING:absl:Found untraced functions such as adapt_step while saving (showing 1 of 1). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s\\files\\model-best\\assets INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s\\files\\model-best\\assets \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s\\files\\model-best)... Done. 0.2s 1419/1419 [==============================] - 104s 71ms/step - loss: 0.0729 - accuracy: 0.9841 - val_loss: 0.0273 - val_accuracy: 0.9930 - _timestamp: 1657167802.0000 - _runtime: 109.0000 Epoch 2/3 1419/1419 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9952 WARNING:absl:Found untraced functions such as adapt_step while saving (showing 1 of 1). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s\\files\\model-best\\assets INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s\\files\\model-best\\assets \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s\\files\\model-best)... Done. 0.1s 1419/1419 [==============================] - 102s 72ms/step - loss: 0.0178 - accuracy: 0.9952 - val_loss: 0.0264 - val_accuracy: 0.9931 - _timestamp: 1657167905.0000 - _runtime: 212.0000 Epoch 3/3 1419/1419 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9956 WARNING:absl:Found untraced functions such as adapt_step while saving (showing 1 of 1). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s\\files\\model-best\\assets INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s\\files\\model-best\\assets \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s\\files\\model-best)... Done. 0.1s 1419/1419 [==============================] - 99s 70ms/step - loss: 0.0159 - accuracy: 0.9956 - val_loss: 0.0261 - val_accuracy: 0.9936 - _timestamp: 1657168005.0000 - _runtime: 312.0000 1 2 # Evaluasi model_1 model_1 . evaluate ( test_dataset ) 1 2 3 4 5 6 7 355/355 [==============================] - 14s 39ms/step - loss: 0.0261 - accuracy: 0.9936 [0.026110293343663216, 0.9935688376426697] 1 2 3 # Membuat prediksi berdasarkan model_1 model_1_pred_prob = model_1 . predict ( test_dataset ) model_1_pred_prob [: 10 ] 1 2 3 4 5 6 7 8 9 10 array([[4.4162782e-05], [7.1573093e-05], [4.8019815e-06], [9.9727100e-01], [9.9964142e-01], [1.0789679e-05], [1.4505963e-04], [4.8167007e-03], [1.1653579e-04], [9.9448615e-01]], dtype=float32) 1 2 3 # Mengkonversi model_1_pred_prob ke dalam label model_1_pred = tf . squeeze ( tf . round ( model_1_pred_prob )) model_1_pred 1 <tf.Tensor: shape=(11351,), dtype=float32, numpy=array([0., 0., 0., ..., 1., 1., 0.], dtype=float32)> 1 2 3 4 # Menghitung metriks dari model_1 model_1_metrik = hitung_metrik ( target = test_label_encode , prediksi = model_1_pred ) model_1_metrik 1 2 3 4 {'akurasi': 0.9935688485595983, 'presisi': 0.9935703064543551, 'recall': 0.9935688485595983, 'f1-score': 0.9935666849784249} Model 2: Transfer Learning pretrained feature exraction menggunakan Universal Sentence Encoder (USE) 1 2 3 4 5 # Download pretrained USE import tensorflow_hub as hub tf_hub_embedding = hub . KerasLayer ( 'https://tfhub.dev/google/universal-sentence-encoder/4' , trainable = False , name = 'universal_sentence_encoder' ) 1 2 3 4 5 6 # Melakukan tes pretrained embedding pada contoh kata kata_acak = random . choice ( train_kata ) print ( f 'Kata acak: \\n { kata_acak } ' ) kata_embed_pretrain = tf_hub_embedding ([ kata_acak ]) print ( f ' \\n Kata setelah embed dengan USE: \\n { kata_embed_pretrain [ 0 ][: 30 ] } \\n ' ) print ( f 'Panjang dari kata setelah embedding: { len ( kata_embed_pretrain [ 0 ]) } ' ) 1 2 3 4 5 6 7 8 9 10 11 Kata acak: BLACK Kata setelah embed dengan USE: [-0.05150617 -0.00596834 0.04953347 0.0329339 0.02531563 -0.0103454 0.05544865 0.01774512 -0.00898479 0.04252742 -0.02970365 -0.03763022 -0.00209826 -0.05936718 -0.0276504 -0.03143836 0.0275419 0.05507992 -0.05129641 -0.02967714 -0.00617658 0.0467336 -0.00554184 0.00347361 -0.04157941 0.01770764 -0.0202241 -0.06021477 0.01153699 0.03923866] Panjang dari kata setelah embedding: 512 1 2 3 4 5 6 7 # Membuat model_2 menggunakan USE inputs = layers . Input ( shape = [], dtype = tf . string , name = 'layer_input' ) layer_embed_pretrained = tf_hub_embedding ( inputs ) x = layers . Conv1D ( filters = 64 , kernel_size = 5 , padding = 'same' , activation = 'relu' )( tf . expand_dims ( layer_embed_pretrained , axis =- 1 )) x = layers . GlobalMaxPooling1D ( name = 'layer_max_pooling' )( x ) outputs = layers . Dense ( 1 , activation = 'sigmoid' , name = 'layer_output' )( x ) model_2 = tf . keras . Model ( inputs = inputs , outputs = outputs , name = 'model_2_Conv1D_USE_embed' ) 1 2 # Ringkasan model_2 model_2 . summary () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Model: \"model_2_Conv1D_USE_embed\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= layer_input (InputLayer) [(None,)] 0 universal_sentence_encoder (None, 512) 256797824 (KerasLayer) tf.expand_dims (TFOpLambda) (None, 512, 1) 0 conv1d (Conv1D) (None, 512, 64) 384 layer_max_pooling (GlobalMa (None, 64) 0 xPooling1D) layer_output (Dense) (None, 1) 65 ================================================================= Total params: 256,798,273 Trainable params: 449 Non-trainable params: 256,797,824 _________________________________________________________________ 1 2 # Plot model_2 plot_model ( model_2 , show_shapes = True ) 1 2 3 4 # Compile model_2 model_2 . compile ( loss = tf . keras . losses . BinaryCrossentropy (), optimizer = tf . keras . optimizers . Adam (), metrics = [ 'accuracy' ]) 1 2 3 4 5 6 7 8 9 10 11 12 # Setup wandb init dan config wb . init ( project = 'ColorSkim' , entity = 'jpao' , name = 'model_2_Conv1D_USE_embed' , config = { 'epochs' : 3 , 'n_layers' : len ( model_2 . layers )}) # Fit model_2 hist_model_2 = model_2 . fit ( train_dataset , epochs = wb . config . epochs , validation_data = test_dataset , callbacks = [ WandbCallback ()]) Finishing last run (ID:15oqfl8s) before initializing another... Waiting for W&B process to finish... (success). table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%} .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% } .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; } Run history: accuracy \u2581\u2588\u2588 epoch \u2581\u2585\u2588 loss \u2588\u2581\u2581 val_accuracy \u2581\u2582\u2588 val_loss \u2588\u2583\u2581 Run summary: GFLOPS 0.0 accuracy 0.99562 best_epoch 2 best_val_loss 0.02611 epoch 2 loss 0.01587 val_accuracy 0.99357 val_loss 0.02611 Synced model_1_Conv1D_embed : https://wandb.ai/jpao/ColorSkim/runs/15oqfl8s Synced 5 W&B file(s), 1 media file(s), 10 artifact file(s) and 1 other file(s) Find logs at: .\\wandb\\run-20220707_112133-15oqfl8s\\logs Successfully finished last run (ID:15oqfl8s). Initializing new run: Tracking run with wandb version 0.12.21 Run data is saved locally in d:\\ColorSkim\\wandb\\run-20220707_112718-298uq3pm Syncing run model_2_Conv1D_USE_embed to Weights & Biases ( docs ) 1 2 3 4 5 6 7 8 9 \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to compute FLOPs for this model. Epoch 1/3 1419/1419 [==============================] - 157s 108ms/step - loss: 0.5851 - accuracy: 0.7125 - val_loss: 0.4620 - val_accuracy: 0.9055 - _timestamp: 1657168206.0000 - _runtime: 162.0000 Epoch 2/3 1419/1419 [==============================] - 150s 106ms/step - loss: 0.3688 - accuracy: 0.9020 - val_loss: 0.3056 - val_accuracy: 0.9027 - _timestamp: 1657168358.0000 - _runtime: 314.0000 Epoch 3/3 1419/1419 [==============================] - 154s 109ms/step - loss: 0.2764 - accuracy: 0.9056 - val_loss: 0.2576 - val_accuracy: 0.9009 - _timestamp: 1657168511.0000 - _runtime: 467.0000 1 2 # Evaluate model_2 model_2 . evaluate ( test_dataset ) 1 2 3 4 5 6 7 355/355 [==============================] - 17s 47ms/step - loss: 0.2576 - accuracy: 0.9009 [0.25761494040489197, 0.9008898138999939] 1 2 3 # Membuat prediksi dengan model_2 model_2_pred_prob = model_2 . predict ( test_dataset ) model_2_pred_prob [: 10 ] 1 2 3 4 5 6 7 8 9 10 array([[0.10366567], [0.07147089], [0.0694409 ], [0.91198367], [0.95933306], [0.13955557], [0.05977627], [0.14464849], [0.07072161], [0.6322903 ]], dtype=float32) 1 2 3 # Mengkonversi model_2 menjadi label format model_2_pred = tf . squeeze ( tf . round ( model_2_pred_prob )) model_2_pred 1 <tf.Tensor: shape=(11351,), dtype=float32, numpy=array([0., 0., 0., ..., 0., 1., 0.], dtype=float32)> 1 2 3 4 # Menghitung hasil metrik dari model_2 model_2_hasil = hitung_metrik ( target = test_label_encode , prediksi = model_2_pred ) model_2_hasil 1 2 3 4 {'akurasi': 0.9008897894458638, 'presisi': 0.9008754342105381, 'recall': 0.9008897894458638, 'f1-score': 0.9003678474557234} Model 3: Menggunakan positional kata dan custom embed dan concatenate layer 1 2 3 4 5 6 7 # Test prediksi dengan model_1 (model_1_Conv1D_embed) class_list = [ 'bukan_warna' , 'warna' ] article = 'PUMA XTG WOVEN PANTS PUMA BLACK-PUMA WHITE' article_list = article . replace ( \"-\" , \" \" ) . split () model_test = tf . squeeze ( tf . round ( model_1 . predict ( article . replace ( \"-\" , \" \" ) . split ()))) for i in range ( 0 , len ( article_list )): print ( f 'Kata: { article_list [ i ] } \\n Prediksi: { class_list [ int ( model_test [ i ])] } \\n\\n ' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Kata: PUMA Prediksi: bukan_warna Kata: XTG Prediksi: bukan_warna Kata: WOVEN Prediksi: bukan_warna Kata: PANTS Prediksi: bukan_warna Kata: PUMA Prediksi: bukan_warna Kata: BLACK Prediksi: warna Kata: PUMA Prediksi: bukan_warna Kata: WHITE Prediksi: warna 1 model_test 1 2 3 4 5 6 7 8 array([[1.5349576e-03], [7.1742781e-04], [1.4221472e-04], [2.2269943e-04], [1.5349576e-03], [9.9965119e-01], [1.5349576e-03], [9.9896502e-01]], dtype=float32) 1","title":"Dokumentasi ColorSkim"},{"location":"ColorSkim_AI/#colorskim-machine-learning-ai","text":"Saat ini item_description untuk artikel ditulis dalam bentuk/format nama_artikel + warna dimana pemisahan nama_artikel dan warna bervariasi antar brand, beberapa menggunakan spasi, dash, garis miring dsbnya. Pembelajaran mesin ini merupakan pembelajaran yang akan menerapkan jaringan saraf buatan (neural network) untuk mempelajari pola penulisan artikel yang bercampur dengan warna untuk mengekstrak warna saja dari artikel. Akan dilakukan beberapa scenario modelling Natural Language Procesing untuk permasalahan sequence to sequence ini. Pada intinya kita akan membagi kalimat ( item_description ) berdasarkan kata per kata dan mengkategorisasikan masing - masing kata ke dalam satu dari dua kategori warna atau bukan_warna (logistik biner). 1 2 3 4 5 6 7 8 9 10 # import modul import tensorflow as tf from tensorflow.python.client import device_lib import pandas as pd import numpy as np import matplotlib.pyplot as plt import wandb as wb from rahasia import API_KEY_WANDB tf . config . run_functions_eagerly ( True ) tf . data . experimental . enable_debug_mode () 1 2 3 4 # cek ketersediaan GPU untuk modeling # NVidia GeForce MX250 - office # NVidia GeForce GTX1060 - home device_lib . list_local_devices ()[ 1 ] 1 2 3 4 5 6 7 8 9 10 11 name: \"/device:GPU:0\" device_type: \"GPU\" memory_limit: 4852809728 locality { bus_id: 1 links { } } incarnation: 17804305938031545614 physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\" xla_global_id: 416903419 1 2 # login ke wandb wb . login ( key = API_KEY_WANDB ) 1 2 3 4 5 6 7 8 9 10 11 Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving. \u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjpao\u001b[0m (\u001b[33mpri-data\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly. \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line. \u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\jPao/.netrc True","title":"ColorSkim Machine Learning AI"},{"location":"ColorSkim_AI/#membaca-data","text":"1 2 3 # Membaca data ke dalam DataFrame pandas data = pd . read_csv ( 'data/setengah_dataset_artikel.csv' ) data [: 10 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } nama_artikel kata label urut_kata total_kata 0 ADISSAGE-BLACK/BLACK/RUNWHT ADISSAGE bukan_warna 1 4 1 ADISSAGE-BLACK/BLACK/RUNWHT BLACK warna 2 4 2 ADISSAGE-BLACK/BLACK/RUNWHT BLACK warna 3 4 3 ADISSAGE-BLACK/BLACK/RUNWHT RUNWHT warna 4 4 4 ADISSAGE-N.NAVY/N.NAVY/RUNWHT ADISSAGE bukan_warna 1 4 5 ADISSAGE-N.NAVY/N.NAVY/RUNWHT N.NAVY warna 2 4 6 ADISSAGE-N.NAVY/N.NAVY/RUNWHT N.NAVY warna 3 4 7 ADISSAGE-N.NAVY/N.NAVY/RUNWHT RUNWHT warna 4 4 8 3 STRIPE D 29.5-BASKETBALL NATURAL 3 bukan_warna 1 6 9 3 STRIPE D 29.5-BASKETBALL NATURAL STRIPE bukan_warna 2 6","title":"Membaca data"},{"location":"ColorSkim_AI/#eksplorasi-data","text":"1 2 # distribusi label dalam data data [ 'label' ] . value_counts () 1 2 3 bukan_warna 34174 warna 22577 Name: label, dtype: int64","title":"Eksplorasi data"},{"location":"ColorSkim_AI/#konversi-data-ke-dalam-train-dan-test","text":"1 2 3 from sklearn.model_selection import train_test_split train_kata , test_kata , train_label , test_label = train_test_split ( data [ 'kata' ] . to_numpy (), data [ 'label' ] . to_numpy (), test_size = 0.2 , random_state = 42 ) train_kata [: 5 ], test_kata [: 5 ], train_label [: 5 ], test_label [: 5 ] 1 2 3 4 5 6 (array(['INVIS', 'SOLRED', 'JR', 'REACT', 'WHITE'], dtype=object), array(['6', 'GA', 'NIKE', 'BLUE', 'BLACK'], dtype=object), array(['bukan_warna', 'warna', 'bukan_warna', 'bukan_warna', 'warna'], dtype=object), array(['bukan_warna', 'bukan_warna', 'bukan_warna', 'warna', 'warna'], dtype=object))","title":"Konversi data ke dalam train dan test"},{"location":"ColorSkim_AI/#konversi-label-ke-dalam-numerik","text":"1 2 3 4 5 from sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder () train_label_encode = label_encoder . fit_transform ( train_label ) test_label_encode = label_encoder . transform ( test_label ) train_label_encode [: 5 ], test_label_encode [: 5 ] 1 (array([0, 1, 0, 0, 1]), array([0, 0, 0, 1, 1]))","title":"Konversi label ke dalam numerik"},{"location":"ColorSkim_AI/#model-0-model-dasar","text":"1 2 3 4 5 6 7 8 9 10 11 12 from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline # Membuat pipeline untuk mengubah kata ke dalam tf-idf model_0 = Pipeline ([ ( \"tf-idf\" , TfidfVectorizer ()), ( \"clf\" , MultinomialNB ()) ]) # Fit pipeline dengan data training model_0 . fit ( X = train_kata , y = train_label_encode ) #sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\u25b8\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\u25be\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;} Pipeline(steps=[('tf-idf', TfidfVectorizer()), ('clf', MultinomialNB())]) In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. Pipeline Pipeline(steps=[('tf-idf', TfidfVectorizer()), ('clf', MultinomialNB())]) TfidfVectorizer TfidfVectorizer() MultinomialNB MultinomialNB() 1 2 # Evaluasi model_0 pada data test model_0 . score ( X = test_kata , y = test_label_encode ) 1 0.9935688485595983 1 2 3 # Membuat prediksi menggunakan data test pred_model_0 = model_0 . predict ( test_kata ) pred_model_0 1 array([0, 0, 0, ..., 1, 1, 0]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # Membuat fungsi dasar untuk menghitung accuray, precision, recall, f1-score from sklearn.metrics import accuracy_score , precision_recall_fscore_support def hitung_metrik ( target , prediksi ): \"\"\" Menghitung accuracy, precision, recall dan f1-score dari model klasifikasi biner Args: target: label yang sebenarnya dalam bentuk 1D array prediksi: label yang diprediksi dalam bentuk 1D array Returns: nilai accuracy, precision, recall dan f1-score dalam bentuk dictionary \"\"\" # Menghitung akurasi model model_akurasi = accuracy_score ( target , prediksi ) # Menghitung precision, recall, f1-score dan support dari model model_presisi , model_recall , model_f1 , _ = precision_recall_fscore_support ( target , prediksi , average = 'weighted' ) hasil_model = { 'akurasi' : model_akurasi , 'presisi' : model_presisi , 'recall' : model_recall , 'f1-score' : model_f1 } return hasil_model 1 2 3 4 # Menghitung metrik dari model_0 model_0_metrik = hitung_metrik ( target = test_label_encode , prediksi = pred_model_0 ) model_0_metrik 1 2 3 4 {'akurasi': 0.9935688485595983, 'presisi': 0.9935690437085363, 'recall': 0.9935688485595983, 'f1-score': 0.9935671438217326}","title":"Model 0: model dasar"},{"location":"ColorSkim_AI/#menyiapkan-data-text-untuk-model-deep-sequence","text":"","title":"Menyiapkan data (text) untuk model deep sequence"},{"location":"ColorSkim_AI/#text-vectorizer-layer","text":"1 2 # jumlah data (kata) dalam train_data len ( train_kata ) 1 45400 1 2 3 # jumlah data unik (kata unik) dalam train_kata jumlah_kata_train = len ( np . unique ( train_kata )) jumlah_kata_train 1 2940 1 2 3 4 5 # Membuat text vectorizer from tensorflow.keras.layers import TextVectorization vectorizer_kata = TextVectorization ( max_tokens = jumlah_kata_train , output_sequence_length = 1 , standardize = 'lower' ) 1 2 # Mengadaptaasikan text vectorizer ke dalam train_kata vectorizer_kata . adapt ( train_kata ) 1 2 3 4 5 # Test vectorizer kata import random target_kata = random . choice ( train_kata ) print ( f 'Kata: \\n { target_kata } \\n ' ) print ( f 'Kata setelah vektorisasi: \\n { vectorizer_kata ([ target_kata ]) } ' ) 1 2 3 4 5 Kata: 5 Kata setelah vektorisasi: [[47]] 1 vectorizer_kata . get_config () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 {'name': 'text_vectorization', 'trainable': True, 'batch_input_shape': (None,), 'dtype': 'string', 'max_tokens': 2940, 'standardize': 'lower', 'split': 'whitespace', 'ngrams': None, 'output_mode': 'int', 'output_sequence_length': 1, 'pad_to_max_tokens': False, 'sparse': False, 'ragged': False, 'vocabulary': None, 'idf_weights': None} 1 2 3 # Jumlah vocabulary dalam vectorizer_kata jumlah_vocab = vectorizer_kata . get_vocabulary () len ( jumlah_vocab ) 1 2938","title":"Text Vectorizer Layer"},{"location":"ColorSkim_AI/#membuat-text-embedding","text":"1 2 3 4 5 6 # Membuat text embedding layer from tensorflow.keras.layers import Embedding kata_embed = Embedding ( input_dim = len ( jumlah_vocab ), output_dim = 64 , mask_zero = True , name = 'layer_token_embedding' ) 1 2 3 4 5 6 7 # Contoh vectorizer dan embedding print ( f 'Kata sebelum vektorisasi: \\n { target_kata } \\n ' ) kata_tervektor = vectorizer_kata ([ target_kata ]) print ( f ' \\n Kata sesudah vektorisasi (sebelum embedding): \\n { kata_tervektor } \\n ' ) kata_terembed = kata_embed ( kata_tervektor ) print ( f ' \\n Kata setelah embedding: \\n { kata_terembed } \\n ' ) print ( f 'Shape dari kata setelah embedding: \\n { kata_terembed . shape } ' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Kata sebelum vektorisasi: 5 Kata sesudah vektorisasi (sebelum embedding): [[47]] Kata setelah embedding: [[[-0.00258959 0.04990998 -0.04441559 -0.02205954 -0.03956609 -0.01799051 -0.02189567 -0.01263437 0.04183039 -0.029005 -0.02622869 0.02765231 -0.04219884 0.00494075 -0.04544568 -0.02989398 0.0400069 -0.03978921 0.03846495 0.01232683 0.0007933 -0.02543398 0.00365927 -0.01668191 0.01294837 0.00481149 -0.02894466 -0.02507013 0.04588108 0.0405197 -0.01860956 -0.04605418 -0.03735595 -0.00483001 -0.00458407 -0.01903701 0.03657718 0.00718967 0.01976332 -0.02251861 0.00589142 -0.00238026 -0.01294807 0.02448275 -0.02560136 0.033471 0.0314525 0.00829456 -0.03963711 0.03865555 0.01974393 0.00157628 -0.01058214 0.04987845 -0.01076373 -0.02952557 0.03197506 -0.01180102 -0.02673483 0.03148599 -0.04402238 0.01858581 -0.01718934 -0.00017769]]] Shape dari kata setelah embedding: (1, 1, 64)","title":"Membuat Text Embedding"},{"location":"ColorSkim_AI/#membuat-tensorflow-dataset","text":"1 2 3 4 5 # Membuat TensorFlow dataset train_dataset = tf . data . Dataset . from_tensor_slices (( train_kata , train_label_encode )) test_dataset = tf . data . Dataset . from_tensor_slices (( test_kata , test_label_encode )) train_dataset 1 <TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))> 1 2 3 4 5 # Membuat TensorSliceDataset menjadi prefetched dataset train_dataset = train_dataset . batch ( 32 ) . prefetch ( tf . data . AUTOTUNE ) test_dataset = test_dataset . batch ( 32 ) . prefetch ( tf . data . AUTOTUNE ) train_dataset 1 <BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>","title":"Membuat TensorFlow Dataset"},{"location":"ColorSkim_AI/#model-1-conv1d-dengan-embedding","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Membuat model_1 dengan layer Conv1D dari kata yang divektorisasi dan di-embed from tensorflow.keras import layers inputs = layers . Input ( shape = ( 1 ,), dtype = tf . string , name = 'layer_input' ) layer_vektor = vectorizer_kata ( inputs ) layer_embed = kata_embed ( layer_vektor ) x = layers . Conv1D ( filters = 64 , kernel_size = 5 , padding = 'same' , activation = 'relu' )( layer_embed ) x = layers . GlobalMaxPooling1D ( name = 'layer_max_pool' )( x ) outputs = layers . Dense ( units = 1 , activation = 'sigmoid' , name = 'layer_output' )( x ) model_1 = tf . keras . Model ( inputs = inputs , outputs = outputs , name = 'model_1_Conv1D_embed' ) # Compile model_1 . compile ( loss = tf . keras . losses . BinaryCrossentropy (), optimizer = tf . keras . optimizers . Adam (), metrics = [ 'accuracy' ]) 1 2 # Ringkasa model_1 model_1 . summary () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Model: \"model_1_Conv1D_embed\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= layer_input (InputLayer) [(None, 1)] 0 text_vectorization (TextVec (None, 1) 0 torization) layer_token_embedding (Embe (None, 1, 64) 188032 dding) conv1d (Conv1D) (None, 1, 64) 20544 layer_max_pool (GlobalMaxPo (None, 64) 0 oling1D) layer_output (Dense) (None, 1) 65 ================================================================= Total params: 208,641 Trainable params: 208,641 Non-trainable params: 0 _________________________________________________________________ 1 2 3 # Plot model_1 from tensorflow.keras.utils import plot_model plot_model ( model_1 , show_shapes = True ) 1 You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # import WandbCallback from wandb.keras import WandbCallback # Setup wandb init dan config wb . init ( project = 'ColorSkim' , entity = 'jpao' , name = 'model_1_Conv1D_embed' , config = { 'epochs' : 3 , 'n_layers' : len ( model_1 . layers )}) # Fit model_1 hist_model_1 = model_1 . fit ( train_dataset , epochs = wb . config . epochs , validation_data = test_dataset , callbacks = [ WandbCallback ()]) 1 \u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjpao\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin Tracking run with wandb version 0.12.21 Run data is saved locally in d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s Syncing run model_1_Conv1D_embed to Weights & Biases ( docs ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts. WARNING:tensorflow:Issue encountered when serializing table_initializer. Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore. 'NoneType' object has no attribute 'name' WARNING:tensorflow:From c:\\Users\\jPao\\miniconda3\\envs\\tf-py39\\lib\\site-packages\\tensorflow\\python\\profiler\\internal\\flops_registry.py:138: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name` Epoch 1/3 1419/1419 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9841 \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as W&B Artifacts in the SavedModel format. WARNING:absl:Found untraced functions such as adapt_step while saving (showing 1 of 1). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s\\files\\model-best\\assets INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s\\files\\model-best\\assets \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s\\files\\model-best)... Done. 0.2s 1419/1419 [==============================] - 104s 71ms/step - loss: 0.0729 - accuracy: 0.9841 - val_loss: 0.0273 - val_accuracy: 0.9930 - _timestamp: 1657167802.0000 - _runtime: 109.0000 Epoch 2/3 1419/1419 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9952 WARNING:absl:Found untraced functions such as adapt_step while saving (showing 1 of 1). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s\\files\\model-best\\assets INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s\\files\\model-best\\assets \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s\\files\\model-best)... Done. 0.1s 1419/1419 [==============================] - 102s 72ms/step - loss: 0.0178 - accuracy: 0.9952 - val_loss: 0.0264 - val_accuracy: 0.9931 - _timestamp: 1657167905.0000 - _runtime: 212.0000 Epoch 3/3 1419/1419 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9956 WARNING:absl:Found untraced functions such as adapt_step while saving (showing 1 of 1). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s\\files\\model-best\\assets INFO:tensorflow:Assets written to: d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s\\files\\model-best\\assets \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ColorSkim\\wandb\\run-20220707_112133-15oqfl8s\\files\\model-best)... Done. 0.1s 1419/1419 [==============================] - 99s 70ms/step - loss: 0.0159 - accuracy: 0.9956 - val_loss: 0.0261 - val_accuracy: 0.9936 - _timestamp: 1657168005.0000 - _runtime: 312.0000 1 2 # Evaluasi model_1 model_1 . evaluate ( test_dataset ) 1 2 3 4 5 6 7 355/355 [==============================] - 14s 39ms/step - loss: 0.0261 - accuracy: 0.9936 [0.026110293343663216, 0.9935688376426697] 1 2 3 # Membuat prediksi berdasarkan model_1 model_1_pred_prob = model_1 . predict ( test_dataset ) model_1_pred_prob [: 10 ] 1 2 3 4 5 6 7 8 9 10 array([[4.4162782e-05], [7.1573093e-05], [4.8019815e-06], [9.9727100e-01], [9.9964142e-01], [1.0789679e-05], [1.4505963e-04], [4.8167007e-03], [1.1653579e-04], [9.9448615e-01]], dtype=float32) 1 2 3 # Mengkonversi model_1_pred_prob ke dalam label model_1_pred = tf . squeeze ( tf . round ( model_1_pred_prob )) model_1_pred 1 <tf.Tensor: shape=(11351,), dtype=float32, numpy=array([0., 0., 0., ..., 1., 1., 0.], dtype=float32)> 1 2 3 4 # Menghitung metriks dari model_1 model_1_metrik = hitung_metrik ( target = test_label_encode , prediksi = model_1_pred ) model_1_metrik 1 2 3 4 {'akurasi': 0.9935688485595983, 'presisi': 0.9935703064543551, 'recall': 0.9935688485595983, 'f1-score': 0.9935666849784249}","title":"Model 1: Conv1D dengan embedding"},{"location":"ColorSkim_AI/#model-2-transfer-learning-pretrained-feature-exraction-menggunakan-universal-sentence-encoder-use","text":"1 2 3 4 5 # Download pretrained USE import tensorflow_hub as hub tf_hub_embedding = hub . KerasLayer ( 'https://tfhub.dev/google/universal-sentence-encoder/4' , trainable = False , name = 'universal_sentence_encoder' ) 1 2 3 4 5 6 # Melakukan tes pretrained embedding pada contoh kata kata_acak = random . choice ( train_kata ) print ( f 'Kata acak: \\n { kata_acak } ' ) kata_embed_pretrain = tf_hub_embedding ([ kata_acak ]) print ( f ' \\n Kata setelah embed dengan USE: \\n { kata_embed_pretrain [ 0 ][: 30 ] } \\n ' ) print ( f 'Panjang dari kata setelah embedding: { len ( kata_embed_pretrain [ 0 ]) } ' ) 1 2 3 4 5 6 7 8 9 10 11 Kata acak: BLACK Kata setelah embed dengan USE: [-0.05150617 -0.00596834 0.04953347 0.0329339 0.02531563 -0.0103454 0.05544865 0.01774512 -0.00898479 0.04252742 -0.02970365 -0.03763022 -0.00209826 -0.05936718 -0.0276504 -0.03143836 0.0275419 0.05507992 -0.05129641 -0.02967714 -0.00617658 0.0467336 -0.00554184 0.00347361 -0.04157941 0.01770764 -0.0202241 -0.06021477 0.01153699 0.03923866] Panjang dari kata setelah embedding: 512 1 2 3 4 5 6 7 # Membuat model_2 menggunakan USE inputs = layers . Input ( shape = [], dtype = tf . string , name = 'layer_input' ) layer_embed_pretrained = tf_hub_embedding ( inputs ) x = layers . Conv1D ( filters = 64 , kernel_size = 5 , padding = 'same' , activation = 'relu' )( tf . expand_dims ( layer_embed_pretrained , axis =- 1 )) x = layers . GlobalMaxPooling1D ( name = 'layer_max_pooling' )( x ) outputs = layers . Dense ( 1 , activation = 'sigmoid' , name = 'layer_output' )( x ) model_2 = tf . keras . Model ( inputs = inputs , outputs = outputs , name = 'model_2_Conv1D_USE_embed' ) 1 2 # Ringkasan model_2 model_2 . summary () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Model: \"model_2_Conv1D_USE_embed\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= layer_input (InputLayer) [(None,)] 0 universal_sentence_encoder (None, 512) 256797824 (KerasLayer) tf.expand_dims (TFOpLambda) (None, 512, 1) 0 conv1d (Conv1D) (None, 512, 64) 384 layer_max_pooling (GlobalMa (None, 64) 0 xPooling1D) layer_output (Dense) (None, 1) 65 ================================================================= Total params: 256,798,273 Trainable params: 449 Non-trainable params: 256,797,824 _________________________________________________________________ 1 2 # Plot model_2 plot_model ( model_2 , show_shapes = True ) 1 2 3 4 # Compile model_2 model_2 . compile ( loss = tf . keras . losses . BinaryCrossentropy (), optimizer = tf . keras . optimizers . Adam (), metrics = [ 'accuracy' ]) 1 2 3 4 5 6 7 8 9 10 11 12 # Setup wandb init dan config wb . init ( project = 'ColorSkim' , entity = 'jpao' , name = 'model_2_Conv1D_USE_embed' , config = { 'epochs' : 3 , 'n_layers' : len ( model_2 . layers )}) # Fit model_2 hist_model_2 = model_2 . fit ( train_dataset , epochs = wb . config . epochs , validation_data = test_dataset , callbacks = [ WandbCallback ()]) Finishing last run (ID:15oqfl8s) before initializing another... Waiting for W&B process to finish... (success). table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%} .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% } .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }","title":"Model 2: Transfer Learning pretrained feature exraction menggunakan Universal Sentence Encoder (USE)"},{"location":"ColorSkim_AI/#model-3-menggunakan-positional-kata-dan-custom-embed-dan-concatenate-layer","text":"1 2 3 4 5 6 7 # Test prediksi dengan model_1 (model_1_Conv1D_embed) class_list = [ 'bukan_warna' , 'warna' ] article = 'PUMA XTG WOVEN PANTS PUMA BLACK-PUMA WHITE' article_list = article . replace ( \"-\" , \" \" ) . split () model_test = tf . squeeze ( tf . round ( model_1 . predict ( article . replace ( \"-\" , \" \" ) . split ()))) for i in range ( 0 , len ( article_list )): print ( f 'Kata: { article_list [ i ] } \\n Prediksi: { class_list [ int ( model_test [ i ])] } \\n\\n ' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Kata: PUMA Prediksi: bukan_warna Kata: XTG Prediksi: bukan_warna Kata: WOVEN Prediksi: bukan_warna Kata: PANTS Prediksi: bukan_warna Kata: PUMA Prediksi: bukan_warna Kata: BLACK Prediksi: warna Kata: PUMA Prediksi: bukan_warna Kata: WHITE Prediksi: warna 1 model_test 1 2 3 4 5 6 7 8 array([[1.5349576e-03], [7.1742781e-04], [1.4221472e-04], [2.2269943e-04], [1.5349576e-03], [9.9965119e-01], [1.5349576e-03], [9.9896502e-01]], dtype=float32) 1","title":"Model 3: Menggunakan positional kata dan custom embed dan concatenate layer"},{"location":"colorskim_ai/","text":"ColorSkim Machine Learning Saat ini item_description untuk artikel ditulis dalam bentuk/format nama_artikel + warna dimana pemisahan nama_artikel dan warna bervariasi antar brand, beberapa menggunakan spasi, dash, garis miring dsbnya. Pembelajaran mesin ini merupakan pembelajaran yang akan menerapkan jaringan saraf buatan (neural network) untuk mempelajari pola penulisan artikel yang bercampur dengan warna untuk mengekstrak warna saja dari artikel. Akan dilakukan beberapa scenario modeling Natural Language Processing untuk permasalahan sequence to sequence ini. Pada intinya kita akan membagi kalimat ( item_description ) berdasarkan kata per kata dan mengkategorikan masing - masing kata ke dalam satu dari 2 kategori warna atau bukan warna (logistik biner). 1 2 3 4 5 6 7 8 9 # Install wandb (weights and biases) ! pip install wandb # Import modul import tensorflow as tf import pandas as pd import numpy as np import matplotlib.pyplot as plt import wandb as wb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.19) Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13) Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0) Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9) Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8) Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0) Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0) Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27) Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3) Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2) Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0) Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.6.0) Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3) Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2) Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1) Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9) Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0) Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3) Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15) 1 2 # wandb login wb . login ( key = '924d78a46727fe1fb5374706bf1b8a158fe73971' ) 1 2 3 4 5 6 7 8 9 10 \u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjpao\u001b[0m (\u001b[33mpri-data\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly. \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line. \u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc True Membaca data 1 2 3 # Membaca data ke dalam DataFrame pandas data = pd . read_csv ( 'colorskim_word_dataset.csv' ) data [: 10 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } kata label urutan_kata total_kata 0 ADISSAGE not_color 1 2 1 BLACK/BLACK/RUNWHT color 2 2 2 ADISSAGE not_color 1 2 3 N.NAVY/N.NAVY/RUNWHT color 2 2 4 3 not_color 1 6 5 STRIPE not_color 2 6 6 D not_color 3 6 7 29.5 not_color 4 6 8 BASKETBALL color 5 6 9 NATURAL color 6 6 .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } const buttonEl = document.querySelector('#df-630f3a48-eb42-4253-8423-c461565d92a8 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-630f3a48-eb42-4253-8423-c461565d92a8'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } Eksplorasi data 1 2 # distribusi label dalam data data [ 'label' ] . value_counts ()[: 20 ] 1 2 3 not_color 439 color 148 Name: label, dtype: int64 1 Konversi data ke dalam train dan test 1 2 3 from sklearn.model_selection import train_test_split train_kata , test_kata , train_label , test_label = train_test_split ( data [ 'kata' ] . to_numpy (), data [ 'label' ] . to_numpy (), test_size = 0.3 ) train_kata [: 5 ], test_kata [: 5 ], train_label [: 5 ], test_label [: 5 ] 1 2 3 4 5 6 7 (array(['CLIMA', 'ULTRABOOST', 'MYSINK/HIRAQU/CROYAL', 'TANGO', 'MILANO'], dtype=object), array(['SCARF', 'AC', 'MGREYH/MGREYH/BLACK', '3P', 'LINEAR'], dtype=object), array(['not_color', 'not_color', 'color', 'not_color', 'not_color'], dtype=object), array(['not_color', 'not_color', 'color', 'not_color', 'not_color'], dtype=object)) Konversi label ke dalam numerik 1 2 3 4 5 from sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder () train_label_encode = label_encoder . fit_transform ( train_label ) test_label_encode = label_encoder . transform ( test_label ) train_label_encode [: 5 ], test_label_encode [: 5 ] 1 (array([1, 1, 0, 1, 1]), array([1, 1, 0, 1, 1])) Model 0: model dasar 1 2 3 4 5 6 7 8 9 10 11 12 13 from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline # Membuat pipeline model_0 = Pipeline ([ ( \"tf-idf\" , TfidfVectorizer ()), ( \"clf\" , MultinomialNB ()) ]) # Fit pipeline ke data training model_0 . fit ( X = train_kata , y = train_label_encode ) 1 Pipeline(steps=[('tf-idf', TfidfVectorizer()), ('clf', MultinomialNB())]) 1 2 3 # Evaluasi model_0 pada data test model_0 . score ( X = test_kata , y = test_label_encode ) 1 0.9661016949152542 1 2 3 # Membuat prediksi menggunakan model_0 pred_model_0 = model_0 . predict ( test_kata ) pred_model_0 1 2 3 4 5 6 7 8 9 array([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Membuat fungsi dasar untuk menghitung accuracy, precision, recall dan f1-score from sklearn.metrics import accuracy_score , precision_recall_fscore_support def hitung_hasil ( target , prediksi ): \"\"\" Menghitung accuracy, precision, recall dan f1-score dari model klasifikasi biner Args: target: label yang sebenarnya dalam bentuk 1D array prediksi: label yang diprediksi dalam bentuk 1D array Returns: nilai accuracy, precision, recall dan f1-score dalam bentuk dictionary \"\"\" # Menghitung akurasi model model_akurasi = accuracy_score ( target , prediksi ) # Menghitung precision, recall dan f1-score model menggunakan \"weighted average\" model_presisi , model_recall , model_f1 , _ = precision_recall_fscore_support ( target , prediksi , average = 'weighted' ) hasil_model = { 'akurasi' : model_akurasi , 'presisi' : model_presisi , 'recall' : model_recall , 'f1-score' : model_f1 } return hasil_model 1 2 3 4 # Menghitung hasil model_0 model_0_hasil = hitung_hasil ( target = test_label_encode , prediksi = pred_model_0 ) model_0_hasil 1 2 3 4 {'akurasi': 0.9661016949152542, 'f1-score': 0.9652353913868151, 'presisi': 0.9675649311059626, 'recall': 0.9661016949152542} Menyiapkan data (text) untuk model deep sequence Text Vectorizer Layer 1 2 # jumlah data (kata) dalam train_kata len ( train_kata ) 1 412 1 2 3 # jumlah data unik (kata unik) dalam train_kata jumlah_kata_train = len ( np . unique ( train_kata )) jumlah_kata_train 1 226 1 2 3 4 5 # Membuat text vectorizer from tensorflow.keras.layers import TextVectorization vectorizer_kata = TextVectorization ( max_tokens = jumlah_kata_train , output_sequence_length = 1 , standardize = 'lower' ) 1 2 # Mengadaptasikan text vectorizer ke dalam train_kata vectorizer_kata . adapt ( train_kata ) 1 2 3 4 5 # Test vectorizer_kata import random target_kata = random . choice ( train_kata ) print ( f 'Kata: \\n { target_kata } \\n ' ) print ( f 'Kata seteleah vektorisasi: \\n { vectorizer_kata ([ target_kata ]) } ' ) 1 2 3 4 5 Kata: EVERLESTO Kata seteleah vektorisasi: [[163]] 1 vectorizer_kata . get_config () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 {'batch_input_shape': (None,), 'dtype': 'string', 'idf_weights': None, 'max_tokens': 226, 'name': 'text_vectorization', 'ngrams': None, 'output_mode': 'int', 'output_sequence_length': 1, 'pad_to_max_tokens': False, 'ragged': False, 'sparse': False, 'split': 'whitespace', 'standardize': 'lower', 'trainable': True, 'vocabulary': None} 1 2 3 # Jumlah vocabulary dalam vectorizer_kata jumlah_vocab = vectorizer_kata . get_vocabulary () len ( jumlah_vocab ) 1 226 Membuat Text Embedding 1 2 3 4 5 6 # Membuat text embedding layer from tensorflow.keras.layers import Embedding kata_embed = Embedding ( input_dim = len ( jumlah_vocab ), output_dim = 64 , mask_zero = True , name = 'layer_token_embedding' ) 1 2 3 4 5 6 7 # Contoh vectorizer dan embedding print ( f 'Kata sebelum vektorisasi: \\n { target_kata } \\n ' ) kata_tervektor = vectorizer_kata ([ target_kata ]) print ( f ' \\n Kata sesudah vektorisasi (sebelum embedding): \\n { kata_tervektor } \\n ' ) kata_terembed = kata_embed ( kata_tervektor ) print ( f ' \\n Kata seteleh embedding: \\n { kata_terembed } \\n ' ) print ( f 'Shape dari kata setelah embedding: { kata_terembed . shape } ' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Kata sebelum vektorisasi: EVERLESTO Kata sesudah vektorisasi (sebelum embedding): [[163]] Kata seteleh embedding: [[[ 0.01631815 -0.04775247 -0.00520905 0.02826596 -0.02610376 -0.02318859 0.04112567 -0.03131066 -0.0055652 0.02334623 -0.00561842 0.00632354 0.02209767 0.02569784 0.00146017 -0.02496719 0.04397715 0.02374946 -0.02793208 -0.02479894 -0.02689627 0.02449668 0.02413115 -0.00026416 -0.0474188 0.02375449 -0.03313603 0.01957679 0.01208953 0.02894038 0.04320562 0.02123917 0.03991547 0.00471902 0.00765711 0.02515994 -0.04454259 -0.0184782 -0.0466426 0.03179142 0.00160015 0.03690684 -0.01465347 -0.00856692 -0.04190071 -0.0354219 0.04571031 -0.04488987 -0.02895441 0.03390359 0.01122769 0.00747364 0.01801378 -0.02941638 -0.03116806 -0.04856103 -0.00071516 -0.01321958 -0.0118474 0.03304395 0.01553127 0.00069499 -0.04673848 0.02494338]]] Shape dari kata setelah embedding: (1, 1, 64) Membuat TensorFlow dataset 1 2 3 4 5 # Membuat TensorFlow dataset train_dataset = tf . data . Dataset . from_tensor_slices (( train_kata , train_label_encode )) test_dataset = tf . data . Dataset . from_tensor_slices (( test_kata , test_label_encode )) train_dataset 1 <TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))> 1 2 3 4 5 # Membuat TensorSliceDataset menjadi prefetced dataset train_dataset = train_dataset . batch ( 32 ) . prefetch ( tf . data . AUTOTUNE ) test_dataset = test_dataset . batch ( 32 ) . prefetch ( tf . data . AUTOTUNE ) train_dataset 1 <PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))> Model 1: Conv1D dengan embedding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Membuat model_1 dengan layer Conv1D dari kata yang divektorisasi dan di-embed from tensorflow.keras import layers inputs = layers . Input ( shape = ( 1 ,), dtype = tf . string , name = 'layer_input' ) layer_vektor = vectorizer_kata ( inputs ) layer_embed = kata_embed ( layer_vektor ) x = layers . Conv1D ( 64 , kernel_size = 5 , padding = 'same' , activation = 'relu' )( layer_embed ) x = layers . GlobalMaxPooling1D ( name = 'layer_max_pool' )( x ) outputs = layers . Dense ( 1 , activation = 'sigmoid' , name = 'layer_output' )( x ) model_1 = tf . keras . Model ( inputs , outputs , name = 'model_1_Conv1D_embed' ) # Compile model_1 . compile ( loss = tf . keras . losses . BinaryCrossentropy (), optimizer = tf . keras . optimizers . Adam (), metrics = [ 'accuracy' ]) 1 2 # Ringkasan model_1 model_1 . summary () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Model: \"model_1_Conv1D_embed\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= layer_input (InputLayer) [(None, 1)] 0 text_vectorization (TextVec (None, 1) 0 torization) layer_token_embedding (Embe (None, 1, 64) 14464 dding) conv1d (Conv1D) (None, 1, 64) 20544 layer_max_pool (GlobalMaxPo (None, 64) 0 oling1D) layer_output (Dense) (None, 1) 65 ================================================================= Total params: 35,073 Trainable params: 35,073 Non-trainable params: 0 _________________________________________________________________ 1 2 3 # Plot model_1 from tensorflow.keras.utils import plot_model plot_model ( model_1 , show_shapes = True ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # import WandbCallback from wandb.keras import WandbCallback # Setup wandb init dan config wb . init ( project = 'ColorSkim' , entity = 'jpao' , name = 'model_1_Conv1D_embed' , config = { 'epochs' : 5 , 'n_layers' : len ( model_1 . layers )}) # Fit model_1 hist_model_1 = model_1 . fit ( train_dataset , epochs = wb . config . epochs , validation_data = test_dataset , callbacks = [ WandbCallback ()]) 1 \u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjpao\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin Tracking run with wandb version 0.12.19 Run data is saved locally in /content/wandb/run-20220627_085711-31cyq9st Syncing run model_1_Conv1D_embed to Weights & Biases ( docs ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts. Epoch 1/5 1/13 [=>............................] - ETA: 14s - loss: 0.6899 - accuracy: 0.7188 \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as W&B Artifacts in the SavedModel format. INFO:tensorflow:Assets written to: /content/wandb/run-20220627_085711-31cyq9st/files/model-best/assets \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20220627_085711-31cyq9st/files/model-best)... Done. 0.1s 13/13 [==============================] - 4s 207ms/step - loss: 0.6785 - accuracy: 0.7354 - val_loss: 0.6626 - val_accuracy: 0.7514 - _timestamp: 1656320235.0000 - _runtime: 4.0000 Epoch 2/5 11/13 [========================>.....] - ETA: 0s - loss: 0.6440 - accuracy: 0.7557INFO:tensorflow:Assets written to: /content/wandb/run-20220627_085711-31cyq9st/files/model-best/assets \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20220627_085711-31cyq9st/files/model-best)... Done. 0.2s 13/13 [==============================] - 3s 211ms/step - loss: 0.6420 - accuracy: 0.7427 - val_loss: 0.6235 - val_accuracy: 0.7514 - _timestamp: 1656320237.0000 - _runtime: 6.0000 Epoch 3/5 8/13 [=================>............] - ETA: 0s - loss: 0.5921 - accuracy: 0.7539INFO:tensorflow:Assets written to: /content/wandb/run-20220627_085711-31cyq9st/files/model-best/assets \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20220627_085711-31cyq9st/files/model-best)... Done. 0.1s 13/13 [==============================] - 3s 231ms/step - loss: 0.5802 - accuracy: 0.7427 - val_loss: 0.5659 - val_accuracy: 0.7514 - _timestamp: 1656320240.0000 - _runtime: 9.0000 Epoch 4/5 13/13 [==============================] - ETA: 0s - loss: 0.4837 - accuracy: 0.7670INFO:tensorflow:Assets written to: /content/wandb/run-20220627_085711-31cyq9st/files/model-best/assets \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20220627_085711-31cyq9st/files/model-best)... Done. 0.2s 13/13 [==============================] - 4s 315ms/step - loss: 0.4837 - accuracy: 0.7670 - val_loss: 0.5023 - val_accuracy: 0.7627 - _timestamp: 1656320245.0000 - _runtime: 14.0000 Epoch 5/5 9/13 [===================>..........] - ETA: 0s - loss: 0.3733 - accuracy: 0.8333INFO:tensorflow:Assets written to: /content/wandb/run-20220627_085711-31cyq9st/files/model-best/assets \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20220627_085711-31cyq9st/files/model-best)... Done. 0.2s 13/13 [==============================] - 3s 276ms/step - loss: 0.3631 - accuracy: 0.8617 - val_loss: 0.4671 - val_accuracy: 0.8192 - _timestamp: 1656320250.0000 - _runtime: 19.0000 1 2 # Evaluasi model_1 model_1 . evaluate ( test_dataset ) 1 2 3 4 5 6 7 6/6 [==============================] - 0s 5ms/step - loss: 0.4671 - accuracy: 0.8192 [0.46710270643234253, 0.8192090392112732] 1 2 3 # Membuat prediksi berdasarkan model_1 model_1_pred_prob = model_1 . predict ( test_dataset ) model_1_pred_prob [: 10 ] 1 2 3 4 5 6 7 8 9 10 array([[0.79627013], [0.856173 ], [0.46073976], [0.8121166 ], [0.79627013], [0.8073617 ], [0.85300666], [0.90371764], [0.4854719 ], [0.79627013]], dtype=float32) 1 2 3 # Mengkonversi model_1_pred_prob dari probabilitas menjadi label model_1_pred = tf . squeeze ( tf . round ( model_1_pred_prob )) model_1_pred 1 2 3 4 5 6 7 8 9 10 11 12 <tf.Tensor: shape=(177,), dtype=float32, numpy= array([1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.], dtype=float32)> 1 2 3 4 # Menghitung metriks model_1 model_1_hasil = hitung_hasil ( target = test_label_encode , prediksi = model_1_pred ) model_1_hasil 1 2 3 4 {'akurasi': 0.8192090395480226, 'f1-score': 0.7772613766243615, 'presisi': 0.8542715288477999, 'recall': 0.8192090395480226} 1 1 array([[0.4478232]], dtype=float32) Model 2: Transfer learning pretrained feature extractor menggunakan Universal Sentence Encoder (USE) 1 2 3 4 5 # Download pretrained USE import tensorflow_hub as hub tf_hub_embedding = hub . KerasLayer ( 'https://tfhub.dev/google/universal-sentence-encoder/4' , trainable = False , name = 'universal_sentence_encoder' ) 1 2 3 4 5 6 # Melakukan tes pretrained embedding dalam pada contoh kata kata_acak = random . choice ( train_kata ) print ( f 'Kata acak: \\n { kata_acak } ' ) kata_embed_pretrain = tf_hub_embedding ([ kata_acak ]) print ( f ' \\n Kata setelah embed dengan USE: \\n { kata_embed_pretrain [ 0 ][: 30 ] } \\n ' ) print ( f 'Panjang dari kata setelah embedding: { len ( kata_embed_pretrain [ 0 ]) } ' ) 1 2 3 4 5 6 7 8 9 10 11 Kata acak: CORBLU/CBLACK/FTWWHT Kata setelah embed dengan USE: [ 0.00860817 -0.00689607 0.05629712 -0.02064848 -0.04581999 0.0858016 0.02770678 -0.04960595 -0.01491964 0.03378397 0.01505858 0.0569918 -0.02326083 0.00949744 -0.06095064 -0.0286258 0.0223882 0.0515826 0.00961048 -0.03192639 0.04371056 -0.00939714 0.01711809 -0.01394025 -0.02168024 0.04030475 -0.01350616 -0.06460485 0.04084557 0.01243608] Panjang dari kata setelah embedding: 512 1 2 3 4 5 6 7 # Membuat model_2 menggunakan USE inputs = layers . Input ( shape = [], dtype = tf . string , name = 'layer_input' ) layer_embed_pretrained = tf_hub_embedding ( inputs ) x = layers . Conv1D ( 64 , kernel_size = 5 , activation = 'relu' , name = 'layer_conv1d' )( tf . expand_dims ( layer_embed_pretrained , axis =- 1 )) x = layers . GlobalMaxPooling1D ( name = 'layer_max_pooling' )( x ) outputs = layers . Dense ( 1 , activation = 'sigmoid' , name = 'layer_output' )( x ) model_2 = tf . keras . Model ( inputs , outputs , name = 'model_2_Conv1D_USE_embed' ) 1 2 # Ringkasan model_2 model_2 . summary () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Model: \"model_2_Conv1D_USE_embed\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= layer_input (InputLayer) [(None,)] 0 universal_sentence_encoder (None, 512) 256797824 (KerasLayer) tf.expand_dims (TFOpLambda) (None, 512, 1) 0 layer_conv1d (Conv1D) (None, 508, 64) 384 layer_max_pooling (GlobalMa (None, 64) 0 xPooling1D) layer_output (Dense) (None, 1) 65 ================================================================= Total params: 256,798,273 Trainable params: 449 Non-trainable params: 256,797,824 _________________________________________________________________ 1 2 # Plot model_2 plot_model ( model_2 , show_shapes = True ) 1 2 3 4 # Compile model_2 model_2 . compile ( loss = tf . keras . losses . BinaryCrossentropy (), optimizer = tf . keras . optimizers . Adam (), metrics = [ 'accuracy' ]) 1 2 3 4 5 6 7 8 9 10 11 12 # Setup wandb init dan config wb . init ( project = 'ColorSkim' , entity = 'jpao' , name = 'model_2_Conv1D_USE_embed' , config = { 'epochs' : 5 , 'n_layers' : len ( model_2 . layers )}) # Fit model_2 hist_model_2 = model_2 . fit ( train_dataset , epochs = wb . config . epochs , validation_data = test_dataset , callbacks = [ WandbCallback ()]) Finishing last run (ID:31cyq9st) before initializing another... Waiting for W&B process to finish... (success). 1 VBox(children=(Label(value='3.186 MB of 3.186 MB uploaded (0.013 MB deduped)\\r'), FloatProgress(value=1.0, max\u2026 table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%} .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% } .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; } Run history: accuracy \u2581\u2581\u2581\u2583\u2588 epoch \u2581\u2583\u2585\u2586\u2588 loss \u2588\u2587\u2586\u2584\u2581 val_accuracy \u2581\u2581\u2581\u2582\u2588 val_loss \u2588\u2587\u2585\u2582\u2581 Run summary: accuracy 0.86165 best_epoch 4 best_val_loss 0.4671 epoch 4 loss 0.36305 val_accuracy 0.81921 val_loss 0.4671 Synced model_1_Conv1D_embed : https://wandb.ai/jpao/ColorSkim/runs/31cyq9st Synced 5 W&B file(s), 1 media file(s), 16 artifact file(s) and 1 other file(s) Find logs at: ./wandb/run-20220627_085711-31cyq9st/logs Successfully finished last run (ID:31cyq9st). Initializing new run: Tracking run with wandb version 0.12.19 Run data is saved locally in /content/wandb/run-20220627_093220-3pr9h21l Syncing run model_2_Conv1D_USE_embed to Weights & Biases ( docs ) 1 2 3 4 5 6 7 8 9 10 Epoch 1/5 13/13 [==============================] - 15s 669ms/step - loss: 0.6708 - accuracy: 0.7427 - val_loss: 0.6512 - val_accuracy: 0.7514 - _timestamp: 1656322355.0000 - _runtime: 11.0000 Epoch 2/5 13/13 [==============================] - 6s 471ms/step - loss: 0.6408 - accuracy: 0.7427 - val_loss: 0.6283 - val_accuracy: 0.7514 - _timestamp: 1656322362.0000 - _runtime: 18.0000 Epoch 3/5 13/13 [==============================] - 6s 521ms/step - loss: 0.6197 - accuracy: 0.7427 - val_loss: 0.6087 - val_accuracy: 0.7514 - _timestamp: 1656322372.0000 - _runtime: 28.0000 Epoch 4/5 13/13 [==============================] - 6s 471ms/step - loss: 0.6012 - accuracy: 0.7427 - val_loss: 0.5919 - val_accuracy: 0.7514 - _timestamp: 1656322378.0000 - _runtime: 34.0000 Epoch 5/5 13/13 [==============================] - 6s 466ms/step - loss: 0.5861 - accuracy: 0.7427 - val_loss: 0.5794 - val_accuracy: 0.7514 - _timestamp: 1656322389.0000 - _runtime: 45.0000 1 2 # Evaluase model_2 model_2 . evaluate ( test_dataset ) 1 2 3 4 5 6 7 6/6 [==============================] - 0s 19ms/step - loss: 0.5794 - accuracy: 0.7514 [0.5794160962104797, 0.7514124512672424] 1 2 3 # Membuat prediksi dengan model_2 model_2_pred_prob = model_2 . predict ( test_dataset ) model_2_pred_prob [: 10 ] 1 2 3 4 5 6 7 8 9 10 array([[0.6672992 ], [0.67215574], [0.67375624], [0.6732286 ], [0.6711316 ], [0.67131513], [0.66906154], [0.67712283], [0.6757687 ], [0.6676929 ]], dtype=float32) 1 2 3 # Mengkonversi model_2 menjadi label format model_2_pred = tf . squeeze ( tf . round ( model_2_pred_prob )) model_2_pred 1 2 3 4 5 6 7 8 9 10 11 12 <tf.Tensor: shape=(177,), dtype=float32, numpy= array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)> 1 2 3 4 # Menghitung hasil metriks dari model_2 model_2_hasil = hitung_hasil ( target = test_label_encode , prediksi = model_2_pred ) model_2_hasil 1 2 3 4 5 6 7 8 9 10 11 /usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) {'akurasi': 0.751412429378531, 'f1-score': 0.64476034262803, 'presisi': 0.5646206390245458, 'recall': 0.751412429378531} Model 3: Menggunakan positional kata dan custom embed dan concatenate layer 1 1 2 3 4 5 6 7 # Test prediksi dengan model_0 Multinomial Naive-Bayes class_list = [ 'warna' , 'bukan_warna' ] article = 'PUMA XTG WOVEN PANTS PUMA BLACK-PUMA WHITE' article_list = article . replace ( \"-\" , \" \" ) . split () model_test = model_0 . predict ( article . replace ( \"-\" , \" \" ) . split ()) for i in range ( 0 , len ( article_list )): print ( f 'Kata: { article_list [ i ] } \\n Prediksi: { class_list [ model_test [ i ]] } \\n\\n ' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Kata: PUMA Prediksi: bukan_warna Kata: XTG Prediksi: bukan_warna Kata: WOVEN Prediksi: bukan_warna Kata: PANTS Prediksi: bukan_warna Kata: PUMA Prediksi: bukan_warna Kata: BLACK Prediksi: warna Kata: PUMA Prediksi: bukan_warna Kata: WHITE Prediksi: warna 1","title":"Colorskim ai"},{"location":"colorskim_ai/#colorskim-machine-learning","text":"Saat ini item_description untuk artikel ditulis dalam bentuk/format nama_artikel + warna dimana pemisahan nama_artikel dan warna bervariasi antar brand, beberapa menggunakan spasi, dash, garis miring dsbnya. Pembelajaran mesin ini merupakan pembelajaran yang akan menerapkan jaringan saraf buatan (neural network) untuk mempelajari pola penulisan artikel yang bercampur dengan warna untuk mengekstrak warna saja dari artikel. Akan dilakukan beberapa scenario modeling Natural Language Processing untuk permasalahan sequence to sequence ini. Pada intinya kita akan membagi kalimat ( item_description ) berdasarkan kata per kata dan mengkategorikan masing - masing kata ke dalam satu dari 2 kategori warna atau bukan warna (logistik biner). 1 2 3 4 5 6 7 8 9 # Install wandb (weights and biases) ! pip install wandb # Import modul import tensorflow as tf import pandas as pd import numpy as np import matplotlib.pyplot as plt import wandb as wb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.19) Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13) Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0) Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9) Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8) Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0) Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0) Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27) Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3) Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2) Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0) Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.6.0) Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3) Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2) Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1) Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9) Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0) Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3) Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15) 1 2 # wandb login wb . login ( key = '924d78a46727fe1fb5374706bf1b8a158fe73971' ) 1 2 3 4 5 6 7 8 9 10 \u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjpao\u001b[0m (\u001b[33mpri-data\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly. \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line. \u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc True","title":"ColorSkim Machine Learning"},{"location":"colorskim_ai/#membaca-data","text":"1 2 3 # Membaca data ke dalam DataFrame pandas data = pd . read_csv ( 'colorskim_word_dataset.csv' ) data [: 10 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } kata label urutan_kata total_kata 0 ADISSAGE not_color 1 2 1 BLACK/BLACK/RUNWHT color 2 2 2 ADISSAGE not_color 1 2 3 N.NAVY/N.NAVY/RUNWHT color 2 2 4 3 not_color 1 6 5 STRIPE not_color 2 6 6 D not_color 3 6 7 29.5 not_color 4 6 8 BASKETBALL color 5 6 9 NATURAL color 6 6 .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } const buttonEl = document.querySelector('#df-630f3a48-eb42-4253-8423-c461565d92a8 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-630f3a48-eb42-4253-8423-c461565d92a8'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); }","title":"Membaca data"},{"location":"colorskim_ai/#eksplorasi-data","text":"1 2 # distribusi label dalam data data [ 'label' ] . value_counts ()[: 20 ] 1 2 3 not_color 439 color 148 Name: label, dtype: int64 1","title":"Eksplorasi data"},{"location":"colorskim_ai/#konversi-data-ke-dalam-train-dan-test","text":"1 2 3 from sklearn.model_selection import train_test_split train_kata , test_kata , train_label , test_label = train_test_split ( data [ 'kata' ] . to_numpy (), data [ 'label' ] . to_numpy (), test_size = 0.3 ) train_kata [: 5 ], test_kata [: 5 ], train_label [: 5 ], test_label [: 5 ] 1 2 3 4 5 6 7 (array(['CLIMA', 'ULTRABOOST', 'MYSINK/HIRAQU/CROYAL', 'TANGO', 'MILANO'], dtype=object), array(['SCARF', 'AC', 'MGREYH/MGREYH/BLACK', '3P', 'LINEAR'], dtype=object), array(['not_color', 'not_color', 'color', 'not_color', 'not_color'], dtype=object), array(['not_color', 'not_color', 'color', 'not_color', 'not_color'], dtype=object))","title":"Konversi data ke dalam train dan test"},{"location":"colorskim_ai/#konversi-label-ke-dalam-numerik","text":"1 2 3 4 5 from sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder () train_label_encode = label_encoder . fit_transform ( train_label ) test_label_encode = label_encoder . transform ( test_label ) train_label_encode [: 5 ], test_label_encode [: 5 ] 1 (array([1, 1, 0, 1, 1]), array([1, 1, 0, 1, 1]))","title":"Konversi label ke dalam numerik"},{"location":"colorskim_ai/#model-0-model-dasar","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline # Membuat pipeline model_0 = Pipeline ([ ( \"tf-idf\" , TfidfVectorizer ()), ( \"clf\" , MultinomialNB ()) ]) # Fit pipeline ke data training model_0 . fit ( X = train_kata , y = train_label_encode ) 1 Pipeline(steps=[('tf-idf', TfidfVectorizer()), ('clf', MultinomialNB())]) 1 2 3 # Evaluasi model_0 pada data test model_0 . score ( X = test_kata , y = test_label_encode ) 1 0.9661016949152542 1 2 3 # Membuat prediksi menggunakan model_0 pred_model_0 = model_0 . predict ( test_kata ) pred_model_0 1 2 3 4 5 6 7 8 9 array([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Membuat fungsi dasar untuk menghitung accuracy, precision, recall dan f1-score from sklearn.metrics import accuracy_score , precision_recall_fscore_support def hitung_hasil ( target , prediksi ): \"\"\" Menghitung accuracy, precision, recall dan f1-score dari model klasifikasi biner Args: target: label yang sebenarnya dalam bentuk 1D array prediksi: label yang diprediksi dalam bentuk 1D array Returns: nilai accuracy, precision, recall dan f1-score dalam bentuk dictionary \"\"\" # Menghitung akurasi model model_akurasi = accuracy_score ( target , prediksi ) # Menghitung precision, recall dan f1-score model menggunakan \"weighted average\" model_presisi , model_recall , model_f1 , _ = precision_recall_fscore_support ( target , prediksi , average = 'weighted' ) hasil_model = { 'akurasi' : model_akurasi , 'presisi' : model_presisi , 'recall' : model_recall , 'f1-score' : model_f1 } return hasil_model 1 2 3 4 # Menghitung hasil model_0 model_0_hasil = hitung_hasil ( target = test_label_encode , prediksi = pred_model_0 ) model_0_hasil 1 2 3 4 {'akurasi': 0.9661016949152542, 'f1-score': 0.9652353913868151, 'presisi': 0.9675649311059626, 'recall': 0.9661016949152542}","title":"Model 0: model dasar"},{"location":"colorskim_ai/#menyiapkan-data-text-untuk-model-deep-sequence","text":"","title":"Menyiapkan data (text) untuk model deep sequence"},{"location":"colorskim_ai/#text-vectorizer-layer","text":"1 2 # jumlah data (kata) dalam train_kata len ( train_kata ) 1 412 1 2 3 # jumlah data unik (kata unik) dalam train_kata jumlah_kata_train = len ( np . unique ( train_kata )) jumlah_kata_train 1 226 1 2 3 4 5 # Membuat text vectorizer from tensorflow.keras.layers import TextVectorization vectorizer_kata = TextVectorization ( max_tokens = jumlah_kata_train , output_sequence_length = 1 , standardize = 'lower' ) 1 2 # Mengadaptasikan text vectorizer ke dalam train_kata vectorizer_kata . adapt ( train_kata ) 1 2 3 4 5 # Test vectorizer_kata import random target_kata = random . choice ( train_kata ) print ( f 'Kata: \\n { target_kata } \\n ' ) print ( f 'Kata seteleah vektorisasi: \\n { vectorizer_kata ([ target_kata ]) } ' ) 1 2 3 4 5 Kata: EVERLESTO Kata seteleah vektorisasi: [[163]] 1 vectorizer_kata . get_config () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 {'batch_input_shape': (None,), 'dtype': 'string', 'idf_weights': None, 'max_tokens': 226, 'name': 'text_vectorization', 'ngrams': None, 'output_mode': 'int', 'output_sequence_length': 1, 'pad_to_max_tokens': False, 'ragged': False, 'sparse': False, 'split': 'whitespace', 'standardize': 'lower', 'trainable': True, 'vocabulary': None} 1 2 3 # Jumlah vocabulary dalam vectorizer_kata jumlah_vocab = vectorizer_kata . get_vocabulary () len ( jumlah_vocab ) 1 226","title":"Text Vectorizer Layer"},{"location":"colorskim_ai/#membuat-text-embedding","text":"1 2 3 4 5 6 # Membuat text embedding layer from tensorflow.keras.layers import Embedding kata_embed = Embedding ( input_dim = len ( jumlah_vocab ), output_dim = 64 , mask_zero = True , name = 'layer_token_embedding' ) 1 2 3 4 5 6 7 # Contoh vectorizer dan embedding print ( f 'Kata sebelum vektorisasi: \\n { target_kata } \\n ' ) kata_tervektor = vectorizer_kata ([ target_kata ]) print ( f ' \\n Kata sesudah vektorisasi (sebelum embedding): \\n { kata_tervektor } \\n ' ) kata_terembed = kata_embed ( kata_tervektor ) print ( f ' \\n Kata seteleh embedding: \\n { kata_terembed } \\n ' ) print ( f 'Shape dari kata setelah embedding: { kata_terembed . shape } ' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Kata sebelum vektorisasi: EVERLESTO Kata sesudah vektorisasi (sebelum embedding): [[163]] Kata seteleh embedding: [[[ 0.01631815 -0.04775247 -0.00520905 0.02826596 -0.02610376 -0.02318859 0.04112567 -0.03131066 -0.0055652 0.02334623 -0.00561842 0.00632354 0.02209767 0.02569784 0.00146017 -0.02496719 0.04397715 0.02374946 -0.02793208 -0.02479894 -0.02689627 0.02449668 0.02413115 -0.00026416 -0.0474188 0.02375449 -0.03313603 0.01957679 0.01208953 0.02894038 0.04320562 0.02123917 0.03991547 0.00471902 0.00765711 0.02515994 -0.04454259 -0.0184782 -0.0466426 0.03179142 0.00160015 0.03690684 -0.01465347 -0.00856692 -0.04190071 -0.0354219 0.04571031 -0.04488987 -0.02895441 0.03390359 0.01122769 0.00747364 0.01801378 -0.02941638 -0.03116806 -0.04856103 -0.00071516 -0.01321958 -0.0118474 0.03304395 0.01553127 0.00069499 -0.04673848 0.02494338]]] Shape dari kata setelah embedding: (1, 1, 64)","title":"Membuat Text Embedding"},{"location":"colorskim_ai/#membuat-tensorflow-dataset","text":"1 2 3 4 5 # Membuat TensorFlow dataset train_dataset = tf . data . Dataset . from_tensor_slices (( train_kata , train_label_encode )) test_dataset = tf . data . Dataset . from_tensor_slices (( test_kata , test_label_encode )) train_dataset 1 <TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))> 1 2 3 4 5 # Membuat TensorSliceDataset menjadi prefetced dataset train_dataset = train_dataset . batch ( 32 ) . prefetch ( tf . data . AUTOTUNE ) test_dataset = test_dataset . batch ( 32 ) . prefetch ( tf . data . AUTOTUNE ) train_dataset 1 <PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>","title":"Membuat TensorFlow dataset"},{"location":"colorskim_ai/#model-1-conv1d-dengan-embedding","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Membuat model_1 dengan layer Conv1D dari kata yang divektorisasi dan di-embed from tensorflow.keras import layers inputs = layers . Input ( shape = ( 1 ,), dtype = tf . string , name = 'layer_input' ) layer_vektor = vectorizer_kata ( inputs ) layer_embed = kata_embed ( layer_vektor ) x = layers . Conv1D ( 64 , kernel_size = 5 , padding = 'same' , activation = 'relu' )( layer_embed ) x = layers . GlobalMaxPooling1D ( name = 'layer_max_pool' )( x ) outputs = layers . Dense ( 1 , activation = 'sigmoid' , name = 'layer_output' )( x ) model_1 = tf . keras . Model ( inputs , outputs , name = 'model_1_Conv1D_embed' ) # Compile model_1 . compile ( loss = tf . keras . losses . BinaryCrossentropy (), optimizer = tf . keras . optimizers . Adam (), metrics = [ 'accuracy' ]) 1 2 # Ringkasan model_1 model_1 . summary () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Model: \"model_1_Conv1D_embed\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= layer_input (InputLayer) [(None, 1)] 0 text_vectorization (TextVec (None, 1) 0 torization) layer_token_embedding (Embe (None, 1, 64) 14464 dding) conv1d (Conv1D) (None, 1, 64) 20544 layer_max_pool (GlobalMaxPo (None, 64) 0 oling1D) layer_output (Dense) (None, 1) 65 ================================================================= Total params: 35,073 Trainable params: 35,073 Non-trainable params: 0 _________________________________________________________________ 1 2 3 # Plot model_1 from tensorflow.keras.utils import plot_model plot_model ( model_1 , show_shapes = True ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # import WandbCallback from wandb.keras import WandbCallback # Setup wandb init dan config wb . init ( project = 'ColorSkim' , entity = 'jpao' , name = 'model_1_Conv1D_embed' , config = { 'epochs' : 5 , 'n_layers' : len ( model_1 . layers )}) # Fit model_1 hist_model_1 = model_1 . fit ( train_dataset , epochs = wb . config . epochs , validation_data = test_dataset , callbacks = [ WandbCallback ()]) 1 \u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjpao\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin Tracking run with wandb version 0.12.19 Run data is saved locally in /content/wandb/run-20220627_085711-31cyq9st Syncing run model_1_Conv1D_embed to Weights & Biases ( docs ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts. Epoch 1/5 1/13 [=>............................] - ETA: 14s - loss: 0.6899 - accuracy: 0.7188 \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as W&B Artifacts in the SavedModel format. INFO:tensorflow:Assets written to: /content/wandb/run-20220627_085711-31cyq9st/files/model-best/assets \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20220627_085711-31cyq9st/files/model-best)... Done. 0.1s 13/13 [==============================] - 4s 207ms/step - loss: 0.6785 - accuracy: 0.7354 - val_loss: 0.6626 - val_accuracy: 0.7514 - _timestamp: 1656320235.0000 - _runtime: 4.0000 Epoch 2/5 11/13 [========================>.....] - ETA: 0s - loss: 0.6440 - accuracy: 0.7557INFO:tensorflow:Assets written to: /content/wandb/run-20220627_085711-31cyq9st/files/model-best/assets \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20220627_085711-31cyq9st/files/model-best)... Done. 0.2s 13/13 [==============================] - 3s 211ms/step - loss: 0.6420 - accuracy: 0.7427 - val_loss: 0.6235 - val_accuracy: 0.7514 - _timestamp: 1656320237.0000 - _runtime: 6.0000 Epoch 3/5 8/13 [=================>............] - ETA: 0s - loss: 0.5921 - accuracy: 0.7539INFO:tensorflow:Assets written to: /content/wandb/run-20220627_085711-31cyq9st/files/model-best/assets \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20220627_085711-31cyq9st/files/model-best)... Done. 0.1s 13/13 [==============================] - 3s 231ms/step - loss: 0.5802 - accuracy: 0.7427 - val_loss: 0.5659 - val_accuracy: 0.7514 - _timestamp: 1656320240.0000 - _runtime: 9.0000 Epoch 4/5 13/13 [==============================] - ETA: 0s - loss: 0.4837 - accuracy: 0.7670INFO:tensorflow:Assets written to: /content/wandb/run-20220627_085711-31cyq9st/files/model-best/assets \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20220627_085711-31cyq9st/files/model-best)... Done. 0.2s 13/13 [==============================] - 4s 315ms/step - loss: 0.4837 - accuracy: 0.7670 - val_loss: 0.5023 - val_accuracy: 0.7627 - _timestamp: 1656320245.0000 - _runtime: 14.0000 Epoch 5/5 9/13 [===================>..........] - ETA: 0s - loss: 0.3733 - accuracy: 0.8333INFO:tensorflow:Assets written to: /content/wandb/run-20220627_085711-31cyq9st/files/model-best/assets \u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20220627_085711-31cyq9st/files/model-best)... Done. 0.2s 13/13 [==============================] - 3s 276ms/step - loss: 0.3631 - accuracy: 0.8617 - val_loss: 0.4671 - val_accuracy: 0.8192 - _timestamp: 1656320250.0000 - _runtime: 19.0000 1 2 # Evaluasi model_1 model_1 . evaluate ( test_dataset ) 1 2 3 4 5 6 7 6/6 [==============================] - 0s 5ms/step - loss: 0.4671 - accuracy: 0.8192 [0.46710270643234253, 0.8192090392112732] 1 2 3 # Membuat prediksi berdasarkan model_1 model_1_pred_prob = model_1 . predict ( test_dataset ) model_1_pred_prob [: 10 ] 1 2 3 4 5 6 7 8 9 10 array([[0.79627013], [0.856173 ], [0.46073976], [0.8121166 ], [0.79627013], [0.8073617 ], [0.85300666], [0.90371764], [0.4854719 ], [0.79627013]], dtype=float32) 1 2 3 # Mengkonversi model_1_pred_prob dari probabilitas menjadi label model_1_pred = tf . squeeze ( tf . round ( model_1_pred_prob )) model_1_pred 1 2 3 4 5 6 7 8 9 10 11 12 <tf.Tensor: shape=(177,), dtype=float32, numpy= array([1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.], dtype=float32)> 1 2 3 4 # Menghitung metriks model_1 model_1_hasil = hitung_hasil ( target = test_label_encode , prediksi = model_1_pred ) model_1_hasil 1 2 3 4 {'akurasi': 0.8192090395480226, 'f1-score': 0.7772613766243615, 'presisi': 0.8542715288477999, 'recall': 0.8192090395480226} 1 1 array([[0.4478232]], dtype=float32)","title":"Model 1: Conv1D dengan embedding"},{"location":"colorskim_ai/#model-2-transfer-learning-pretrained-feature-extractor-menggunakan-universal-sentence-encoder-use","text":"1 2 3 4 5 # Download pretrained USE import tensorflow_hub as hub tf_hub_embedding = hub . KerasLayer ( 'https://tfhub.dev/google/universal-sentence-encoder/4' , trainable = False , name = 'universal_sentence_encoder' ) 1 2 3 4 5 6 # Melakukan tes pretrained embedding dalam pada contoh kata kata_acak = random . choice ( train_kata ) print ( f 'Kata acak: \\n { kata_acak } ' ) kata_embed_pretrain = tf_hub_embedding ([ kata_acak ]) print ( f ' \\n Kata setelah embed dengan USE: \\n { kata_embed_pretrain [ 0 ][: 30 ] } \\n ' ) print ( f 'Panjang dari kata setelah embedding: { len ( kata_embed_pretrain [ 0 ]) } ' ) 1 2 3 4 5 6 7 8 9 10 11 Kata acak: CORBLU/CBLACK/FTWWHT Kata setelah embed dengan USE: [ 0.00860817 -0.00689607 0.05629712 -0.02064848 -0.04581999 0.0858016 0.02770678 -0.04960595 -0.01491964 0.03378397 0.01505858 0.0569918 -0.02326083 0.00949744 -0.06095064 -0.0286258 0.0223882 0.0515826 0.00961048 -0.03192639 0.04371056 -0.00939714 0.01711809 -0.01394025 -0.02168024 0.04030475 -0.01350616 -0.06460485 0.04084557 0.01243608] Panjang dari kata setelah embedding: 512 1 2 3 4 5 6 7 # Membuat model_2 menggunakan USE inputs = layers . Input ( shape = [], dtype = tf . string , name = 'layer_input' ) layer_embed_pretrained = tf_hub_embedding ( inputs ) x = layers . Conv1D ( 64 , kernel_size = 5 , activation = 'relu' , name = 'layer_conv1d' )( tf . expand_dims ( layer_embed_pretrained , axis =- 1 )) x = layers . GlobalMaxPooling1D ( name = 'layer_max_pooling' )( x ) outputs = layers . Dense ( 1 , activation = 'sigmoid' , name = 'layer_output' )( x ) model_2 = tf . keras . Model ( inputs , outputs , name = 'model_2_Conv1D_USE_embed' ) 1 2 # Ringkasan model_2 model_2 . summary () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Model: \"model_2_Conv1D_USE_embed\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= layer_input (InputLayer) [(None,)] 0 universal_sentence_encoder (None, 512) 256797824 (KerasLayer) tf.expand_dims (TFOpLambda) (None, 512, 1) 0 layer_conv1d (Conv1D) (None, 508, 64) 384 layer_max_pooling (GlobalMa (None, 64) 0 xPooling1D) layer_output (Dense) (None, 1) 65 ================================================================= Total params: 256,798,273 Trainable params: 449 Non-trainable params: 256,797,824 _________________________________________________________________ 1 2 # Plot model_2 plot_model ( model_2 , show_shapes = True ) 1 2 3 4 # Compile model_2 model_2 . compile ( loss = tf . keras . losses . BinaryCrossentropy (), optimizer = tf . keras . optimizers . Adam (), metrics = [ 'accuracy' ]) 1 2 3 4 5 6 7 8 9 10 11 12 # Setup wandb init dan config wb . init ( project = 'ColorSkim' , entity = 'jpao' , name = 'model_2_Conv1D_USE_embed' , config = { 'epochs' : 5 , 'n_layers' : len ( model_2 . layers )}) # Fit model_2 hist_model_2 = model_2 . fit ( train_dataset , epochs = wb . config . epochs , validation_data = test_dataset , callbacks = [ WandbCallback ()]) Finishing last run (ID:31cyq9st) before initializing another... Waiting for W&B process to finish... (success). 1 VBox(children=(Label(value='3.186 MB of 3.186 MB uploaded (0.013 MB deduped)\\r'), FloatProgress(value=1.0, max\u2026 table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%} .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% } .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }","title":"Model 2: Transfer learning pretrained feature extractor menggunakan Universal Sentence Encoder (USE)"},{"location":"colorskim_ai/#model-3-menggunakan-positional-kata-dan-custom-embed-dan-concatenate-layer","text":"1 1 2 3 4 5 6 7 # Test prediksi dengan model_0 Multinomial Naive-Bayes class_list = [ 'warna' , 'bukan_warna' ] article = 'PUMA XTG WOVEN PANTS PUMA BLACK-PUMA WHITE' article_list = article . replace ( \"-\" , \" \" ) . split () model_test = model_0 . predict ( article . replace ( \"-\" , \" \" ) . split ()) for i in range ( 0 , len ( article_list )): print ( f 'Kata: { article_list [ i ] } \\n Prediksi: { class_list [ model_test [ i ]] } \\n\\n ' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Kata: PUMA Prediksi: bukan_warna Kata: XTG Prediksi: bukan_warna Kata: WOVEN Prediksi: bukan_warna Kata: PANTS Prediksi: bukan_warna Kata: PUMA Prediksi: bukan_warna Kata: BLACK Prediksi: warna Kata: PUMA Prediksi: bukan_warna Kata: WHITE Prediksi: warna 1","title":"Model 3: Menggunakan positional kata dan custom embed dan concatenate layer"}]}